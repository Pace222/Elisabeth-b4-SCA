{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","os.environ['CUDA_DEVICE_ORDER'] = \"PCI_BUS_ID\"\n","os.environ['CUDA_VISIBLE_DEVICES'] = \"7\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pickle as pic\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import deep_learning"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["if True:\n","    with open(\"splitted_wavelets_haar_2.pic\", \"rb\") as r:\n","        X_training, X_val, X_extraction = pic.load(r)\n","    with open(\"splitted_labels_739094.pic\", \"rb\") as r:\n","        y_training, y_val, y_extraction = pic.load(r)\n","else:\n","    with open(\"wavelets_haar_2.pic\", \"rb\") as r:\n","        wavelets = pic.load(r)\n","    with open(\"labels_739094.pic\", \"rb\") as r:\n","        rws_perms_labels, round_perms_labels, copy_perms_labels, rws_masks_labels, round_masks_labels = pic.load(r)\n","\n","    X_total, y_total = deep_learning.prepare_data_dl(wavelets, round_perms_labels, copy_perms_labels, round_masks_labels, rws_perms_labels, rws_masks_labels)\n","\n","    profile, test = train_test_split(np.arange(X_total.shape[0]), train_size=500_000, random_state=0)\n","\n","    X_profiling, X_extraction = X_total[profile], X_total[test]\n","    train, val = train_test_split(np.arange(X_profiling.shape[0]), test_size=0.1, random_state=0)\n","    X_training, X_val = X_profiling[train], X_profiling[val]\n","\n","    with open(\"splitted_wavelets_haar_2.pic\", \"wb\") as w:\n","        pic.dump((X_training, X_val, X_extraction), w)\n","\n","    if False:\n","        y_profiling = {}\n","        y_training = {}\n","        y_val = {}\n","        y_extraction = {}\n","        for label in y_total.keys():\n","            print(label, end=\"\\r\")\n","            y_profiling[label], y_extraction[label] = y_total[label][profile], y_total[label][test]\n","            y_training[label], y_val[label] = y_profiling[label][train], y_profiling[label][val]\n","\n","        with open(\"splitted_labels_739094.pic\", \"wb\") as w:\n","            pic.dump((y_training, y_val, y_extraction), w)\n","    else:\n","        with open(\"splitted_labels_739094.pic\", \"rb\") as r:\n","            y_training, y_val, y_extraction = pic.load(r)\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["resnet = deep_learning.ResNetSCA(network=\"haar_2_rws_and_14_rounds\", epochs=1000, dataset_size=X_training.shape[0])\n","try:\n","    deep_learning.check_file_exists(\"./resnet_models/resnet_500000_haar_2.keras\")\n","    from tensorflow.keras.models import load_model\n","    resnet.model = load_model(\"./resnet_models/resnet_500000_haar_2.keras\")\n","except ValueError:\n","    pass"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["train_gen = deep_learning.DataGenerator(X_training, y_training)\n","val_gen = deep_learning.DataGenerator(X_val, y_val)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1000\n"," 469/3516 [===>..........................] - ETA: 1:38:54 - loss: 1096.1825 - rws_perm_output_loss: 0.0321 - rws_mask_0_0_output_loss: 2.7418 - rws_mask_0_1_output_loss: 2.7451 - rws_mask_1_0_output_loss: 2.7449 - rws_mask_1_1_output_loss: 2.7474 - rws_mask_2_0_output_loss: 2.7465 - rws_mask_2_1_output_loss: 2.7469 - rws_mask_3_0_output_loss: 2.7456 - rws_mask_3_1_output_loss: 2.7486 - rws_mask_4_0_output_loss: 2.7396 - rws_mask_4_1_output_loss: 2.7442 - rws_mask_5_0_output_loss: 2.7410 - rws_mask_5_1_output_loss: 2.7463 - rws_mask_6_0_output_loss: 2.7426 - rws_mask_6_1_output_loss: 2.7489 - rws_mask_7_0_output_loss: 2.7446 - rws_mask_7_1_output_loss: 2.7483 - rws_mask_8_0_output_loss: 2.7465 - rws_mask_8_1_output_loss: 2.7478 - rws_mask_9_0_output_loss: 2.7416 - rws_mask_9_1_output_loss: 2.7466 - rws_mask_10_0_output_loss: 2.7409 - rws_mask_10_1_output_loss: 2.7452 - rws_mask_11_0_output_loss: 2.7424 - rws_mask_11_1_output_loss: 2.7479 - rws_mask_12_0_output_loss: 2.7434 - rws_mask_12_1_output_loss: 2.7458 - rws_mask_13_0_output_loss: 2.7489 - rws_mask_13_1_output_loss: 2.7483 - rws_mask_14_0_output_loss: 2.7419 - rws_mask_14_1_output_loss: 2.7451 - rws_mask_15_0_output_loss: 2.7417 - rws_mask_15_1_output_loss: 2.7460 - rws_mask_16_0_output_loss: 2.7418 - rws_mask_16_1_output_loss: 2.7487 - rws_mask_17_0_output_loss: 2.7435 - rws_mask_17_1_output_loss: 2.7473 - rws_mask_18_0_output_loss: 2.7472 - rws_mask_18_1_output_loss: 2.7481 - rws_mask_19_0_output_loss: 2.7434 - rws_mask_19_1_output_loss: 2.7475 - rws_mask_20_0_output_loss: 2.7413 - rws_mask_20_1_output_loss: 2.7447 - rws_mask_21_0_output_loss: 2.7391 - rws_mask_21_1_output_loss: 2.7473 - rws_mask_22_0_output_loss: 2.7447 - rws_mask_22_1_output_loss: 2.7477 - rws_mask_23_0_output_loss: 2.7484 - rws_mask_23_1_output_loss: 2.7481 - rws_mask_24_0_output_loss: 2.7430 - rws_mask_24_1_output_loss: 2.7487 - rws_mask_25_0_output_loss: 2.7435 - rws_mask_25_1_output_loss: 2.7476 - rws_mask_26_0_output_loss: 2.7391 - rws_mask_26_1_output_loss: 2.7493 - rws_mask_27_0_output_loss: 2.7442 - rws_mask_27_1_output_loss: 2.7480 - rws_mask_28_0_output_loss: 2.7474 - rws_mask_28_1_output_loss: 2.7485 - rws_mask_29_0_output_loss: 2.7424 - rws_mask_29_1_output_loss: 2.7473 - rws_mask_30_0_output_loss: 2.7419 - rws_mask_30_1_output_loss: 2.7459 - rws_mask_31_0_output_loss: 2.7395 - rws_mask_31_1_output_loss: 2.7482 - rws_mask_32_0_output_loss: 2.7453 - rws_mask_32_1_output_loss: 2.7465 - rws_mask_33_0_output_loss: 2.7496 - rws_mask_33_1_output_loss: 2.7492 - rws_mask_34_0_output_loss: 2.7434 - rws_mask_34_1_output_loss: 2.7489 - rws_mask_35_0_output_loss: 2.7426 - rws_mask_35_1_output_loss: 2.7450 - rws_mask_36_0_output_loss: 2.7381 - rws_mask_36_1_output_loss: 2.7479 - rws_mask_37_0_output_loss: 2.7428 - rws_mask_37_1_output_loss: 2.7455 - rws_mask_38_0_output_loss: 2.7477 - rws_mask_38_1_output_loss: 2.7488 - rws_mask_39_0_output_loss: 2.7442 - rws_mask_39_1_output_loss: 2.7470 - rws_mask_40_0_output_loss: 2.7437 - rws_mask_40_1_output_loss: 2.7455 - rws_mask_41_0_output_loss: 2.7345 - rws_mask_41_1_output_loss: 2.7476 - rws_mask_42_0_output_loss: 2.7439 - rws_mask_42_1_output_loss: 2.7458 - rws_mask_43_0_output_loss: 2.7476 - rws_mask_43_1_output_loss: 2.7480 - rws_mask_44_0_output_loss: 2.7436 - rws_mask_44_1_output_loss: 2.7490 - rws_mask_45_0_output_loss: 2.7436 - rws_mask_45_1_output_loss: 2.7492 - rws_mask_46_0_output_loss: 2.7365 - rws_mask_46_1_output_loss: 2.7468 - rws_mask_47_0_output_loss: 2.7416 - rws_mask_47_1_output_loss: 2.7479 - rws_mask_48_0_output_loss: 2.7447 - rws_mask_48_1_output_loss: 2.7491 - rws_mask_49_0_output_loss: 2.7461 - rws_mask_49_1_output_loss: 2.7468 - rws_mask_50_0_output_loss: 2.7436 - rws_mask_50_1_output_loss: 2.7489 - rws_mask_51_0_output_loss: 2.7370 - rws_mask_51_1_output_loss: 2.7478 - rws_mask_52_0_output_loss: 2.7412 - rws_mask_52_1_output_loss: 2.7462 - rws_mask_53_0_output_loss: 2.7447 - rws_mask_53_1_output_loss: 2.7477 - rws_mask_54_0_output_loss: 2.7460 - rws_mask_54_1_output_loss: 2.7470 - rws_mask_55_0_output_loss: 2.7459 - rws_mask_55_1_output_loss: 2.7481 - rws_mask_56_0_output_loss: 2.7394 - rws_mask_56_1_output_loss: 2.7449 - rws_mask_57_0_output_loss: 2.7424 - rws_mask_57_1_output_loss: 2.7458 - rws_mask_58_0_output_loss: 2.7452 - rws_mask_58_1_output_loss: 2.7488 - rws_mask_59_0_output_loss: 2.7457 - rws_mask_59_1_output_loss: 2.7474 - rws_mask_60_0_output_loss: 2.7461 - rws_mask_60_1_output_loss: 2.7471 - rws_mask_61_0_output_loss: 2.7376 - rws_mask_61_1_output_loss: 2.7471 - rws_mask_62_0_output_loss: 2.7445 - rws_mask_62_1_output_loss: 2.7451 - rws_mask_63_0_output_loss: 2.7459 - rws_mask_63_1_output_loss: 2.7493 - rws_mask_64_0_output_loss: 2.7449 - rws_mask_64_1_output_loss: 2.7463 - rws_mask_65_0_output_loss: 2.7451 - rws_mask_65_1_output_loss: 2.7474 - rws_mask_66_0_output_loss: 2.7390 - rws_mask_66_1_output_loss: 2.7473 - rws_mask_67_0_output_loss: 2.7410 - rws_mask_67_1_output_loss: 2.7450 - rws_mask_68_0_output_loss: 2.7438 - rws_mask_68_1_output_loss: 2.7470 - rws_mask_69_0_output_loss: 2.7442 - rws_mask_69_1_output_loss: 2.7467 - rws_mask_70_0_output_loss: 2.7445 - rws_mask_70_1_output_loss: 2.7459 - rws_mask_71_0_output_loss: 2.7386 - rws_mask_71_1_output_loss: 2.7466 - rws_mask_72_0_output_loss: 2.7408 - rws_mask_72_1_output_loss: 2.7449 - rws_mask_73_0_output_loss: 2.7421 - rws_mask_73_1_output_loss: 2.7487 - rws_mask_74_0_output_loss: 2.7432 - rws_mask_74_1_output_loss: 2.7488 - rws_mask_75_0_output_loss: 2.7479 - rws_mask_75_1_output_loss: 2.7474 - rws_mask_76_0_output_loss: 2.7393 - rws_mask_76_1_output_loss: 2.7479 - rws_mask_77_0_output_loss: 2.7430 - rws_mask_77_1_output_loss: 2.7446 - rws_mask_78_0_output_loss: 2.7413 - rws_mask_78_1_output_loss: 2.7487 - rws_mask_79_0_output_loss: 2.7443 - rws_mask_79_1_output_loss: 2.7459 - rws_mask_80_0_output_loss: 2.7467 - rws_mask_80_1_output_loss: 2.7457 - rws_mask_81_0_output_loss: 2.7410 - rws_mask_81_1_output_loss: 2.7470 - rws_mask_82_0_output_loss: 2.7408 - rws_mask_82_1_output_loss: 2.7466 - rws_mask_83_0_output_loss: 2.7408 - rws_mask_83_1_output_loss: 2.7489 - rws_mask_84_0_output_loss: 2.7451 - rws_mask_84_1_output_loss: 2.7462 - rws_mask_85_0_output_loss: 2.7477 - rws_mask_85_1_output_loss: 2.7475 - rws_mask_86_0_output_loss: 2.7419 - rws_mask_86_1_output_loss: 2.7474 - rws_mask_87_0_output_loss: 2.7414 - rws_mask_87_1_output_loss: 2.7462 - rws_mask_88_0_output_loss: 2.7387 - rws_mask_88_1_output_loss: 2.7488 - rws_mask_89_0_output_loss: 2.7456 - rws_mask_89_1_output_loss: 2.7459 - rws_mask_90_0_output_loss: 2.7467 - rws_mask_90_1_output_loss: 2.7470 - rws_mask_91_0_output_loss: 2.7430 - rws_mask_91_1_output_loss: 2.7469 - rws_mask_92_0_output_loss: 2.7417 - rws_mask_92_1_output_loss: 2.7451 - rws_mask_93_0_output_loss: 2.7387 - rws_mask_93_1_output_loss: 2.7471 - rws_mask_94_0_output_loss: 2.7434 - rws_mask_94_1_output_loss: 2.7474 - rws_mask_95_0_output_loss: 2.7473 - rws_mask_95_1_output_loss: 2.7477 - rws_mask_96_0_output_loss: 2.7436 - rws_mask_96_1_output_loss: 2.7455 - rws_mask_97_0_output_loss: 2.7304 - rws_mask_97_1_output_loss: 2.7399 - round_perm_output_loss: 0.4545 - block_perm_0_output_loss: 1.3777 - block_perm_1_output_loss: 1.4787 - block_perm_2_output_loss: 1.3718 - block_perm_3_output_loss: 1.4786 - block_perm_4_output_loss: 1.3817 - block_perm_5_output_loss: 1.4806 - block_perm_6_output_loss: 1.3772 - block_perm_7_output_loss: 1.4865 - block_perm_8_output_loss: 1.3791 - block_perm_9_output_loss: 1.4736 - block_perm_10_output_loss: 1.3792 - block_perm_11_output_loss: 1.4786 - block_perm_12_output_loss: 1.3808 - block_perm_13_output_loss: 1.4722 - mask_0_0_0_output_loss: 2.7412 - mask_0_0_1_output_loss: 2.7391 - mask_0_1_0_output_loss: 2.7502 - mask_0_1_1_output_loss: 2.7490 - mask_0_2_0_output_loss: 2.7452 - mask_0_2_1_output_loss: 2.7453 - mask_0_3_0_output_loss: 2.7447 - mask_0_3_1_output_loss: 2.7422 - mask_0_4_0_output_loss: 2.7395 - mask_0_4_1_output_loss: 2.7355 - mask_0_5_0_output_loss: 2.7443 - mask_0_5_1_output_loss: 2.7412 - mask_0_6_0_output_loss: 2.7426 - mask_0_6_1_output_loss: 2.7432 - mask_1_0_0_output_loss: 2.7420 - mask_1_0_1_output_loss: 2.7381 - mask_1_1_0_output_loss: 2.7444 - mask_1_1_1_output_loss: 2.7387 - mask_1_2_0_output_loss: 2.7425 - mask_1_2_1_output_loss: 2.7380 - mask_1_3_0_output_loss: 2.7505 - mask_1_3_1_output_loss: 2.7479 - mask_1_4_0_output_loss: 2.7488 - mask_1_4_1_output_loss: 2.7467 - mask_1_5_0_output_loss: 2.7419 - mask_1_5_1_output_loss: 2.7381 - mask_1_6_0_output_loss: 2.7411 - mask_1_6_1_output_loss: 2.7401 - mask_2_0_0_output_loss: 2.7402 - mask_2_0_1_output_loss: 2.7378 - mask_2_1_0_output_loss: 2.7516 - mask_2_1_1_output_loss: 2.7498 - mask_2_2_0_output_loss: 2.7436 - mask_2_2_1_output_loss: 2.7430 - mask_2_3_0_output_loss: 2.7434 - mask_2_3_1_output_loss: 2.7425 - mask_2_4_0_output_loss: 2.7408 - mask_2_4_1_output_loss: 2.7354 - mask_2_5_0_output_loss: 2.7464 - mask_2_5_1_output_loss: 2.7405 - mask_2_6_0_output_loss: 2.7451 - mask_2_6_1_output_loss: 2.7409 - mask_3_0_0_output_loss: 2.7437 - mask_3_0_1_output_loss: 2.7418 - mask_3_1_0_output_loss: 2.7432 - mask_3_1_1_output_loss: 2.7378 - mask_3_2_0_output_loss: 2.7429 - mask_3_2_1_output_loss: 2.7386 - mask_3_3_0_output_loss: 2.7487 - mask_3_3_1_output_loss: 2.7472 - mask_3_4_0_output_loss: 2.7502 - mask_3_4_1_output_loss: 2.7471 - mask_3_5_0_output_loss: 2.7421 - mask_3_5_1_output_loss: 2.7374 - mask_3_6_0_output_loss: 2.7435 - mask_3_6_1_output_loss: 2.7417 - mask_4_0_0_output_loss: 2.7395 - mask_4_0_1_output_loss: 2.7368 - mask_4_1_0_output_loss: 2.7509 - mask_4_1_1_output_loss: 2.7487 - mask_4_2_0_output_loss: 2.7461 - mask_4_2_1_output_loss: 2.7453 - mask_4_3_0_output_loss: 2.7447 - mask_4_3_1_output_loss: 2.7418 - mask_4_4_0_output_loss: 2.7411 - mask_4_4_1_output_loss: 2.7390 - mask_4_5_0_output_loss: 2.7483 - mask_4_5_1_output_loss: 2.7407 - mask_4_6_0_output_loss: 2.7452 - mask_4_6_1_output_loss: 2.7447 - mask_5_0_0_output_loss: 2.7431 - mask_5_0_1_output_loss: 2.7398 - mask_5_1_0_output_loss: 2.7439 - mask_5_1_1_output_loss: 2.7368 - mask_5_2_0_output_loss: 2.7419 - mask_5_2_1_output_loss: 2.7386 - mask_5_3_0_output_loss: 2.7497 - mask_5_3_1_output_loss: 2.7467 - mask_5_4_0_output_loss: 2.7503 - mask_5_4_1_output_loss: 2.7488 - mask_5_5_0_output_loss: 2.7402 - mask_5_5_1_output_loss: 2.7395 - mask_5_6_0_output_loss: 2.7442 - mask_5_6_1_output_loss: 2.7403 - mask_6_0_0_output_loss: 2.7393 - mask_6_0_1_output_loss: 2.7354 - mask_6_1_0_output_loss: 2.7515 - mask_6_1_1_output_loss: 2.7484 - mask_6_2_0_output_loss: 2.7460 - mask_6_2_1_output_loss: 2.7429 - mask_6_3_0_output_loss: 2.7429 - mask_6_3_1_output_loss: 2.7413 - mask_6_4_0_output_loss: 2.7407 - mask_6_4_1_output_loss: 2.7376 - mask_6_5_0_output_loss: 2.7454 - mask_6_5_1_output_loss: 2.7395 - mask_6_6_0_output_loss: 2.7455 - mask_6_6_1_output_loss: 2.7414 - mask_7_0_0_output_loss: 2.7433 - mask_7_0_1_output_loss: 2.7417 - mask_7_1_0_output_loss: 2.7452 - mask_7_1_1_output_loss: 2.7386 - mask_7_2_0_output_loss: 2.7417 - mask_7_2_1_output_loss: 2.7363 - mask_7_3_0_output_loss: 2.7500 - mask_7_3_1_output_loss: 2.7460 - mask_7_4_0_output_loss: 2.7505 - mask_7_4_1_output_loss: 2.7478 - mask_7_5_0_output_loss: 2.7420 - mask_7_5_1_output_loss: 2.7388 - mask_7_6_0_output_loss: 2.7435 - mask_7_6_1_output_loss: 2.7433 - mask_8_0_0_output_loss: 2.7369 - mask_8_0_1_output_loss: 2.7364 - mask_8_1_0_output_loss: 2.7515 - mask_8_1_1_output_loss: 2.7483 - mask_8_2_0_output_loss: 2.7448 - mask_8_2_1_output_loss: 2.7446 - mask_8_3_0_output_loss: 2.7434 - mask_8_3_1_output_loss: 2.7422 - mask_8_4_0_output_loss: 2.7408 - mask_8_4_1_output_loss: 2.7342 - mask_8_5_0_output_loss: 2.7451 - mask_8_5_1_output_loss: 2.7428 - mask_8_6_0_output_loss: 2.7452 - mask_8_6_1_output_loss: 2.7440 - mask_9_0_0_output_loss: 2.7426 - mask_9_0_1_output_loss: 2.7427 - mask_9_1_0_output_loss: 2.7459 - mask_9_1_1_output_loss: 2.7405 - mask_9_2_0_output_loss: 2.7414 - mask_9_2_1_output_loss: 2.7370 - mask_9_3_0_output_loss: 2.7496 - mask_9_3_1_output_loss: 2.7483 - mask_9_4_0_output_loss: 2.7510 - mask_9_4_1_output_loss: 2.7507 - mask_9_5_0_output_loss: 2.7417 - mask_9_5_1_output_loss: 2.7411 - mask_9_6_0_output_loss: 2.7441 - mask_9_6_1_output_loss: 2.7410 - mask_10_0_0_output_loss: 2.7386 - mask_10_0_1_output_loss: 2.7335 - mask_10_1_0_output_loss: 2.7516 - mask_10_1_1_output_loss: 2.7490 - mask_10_2_0_output_loss: 2.7450 - mask_10_2_1_output_loss: 2.7462 - mask_10_3_0_output_loss: 2.7437 - mask_10_3_1_output_loss: 2.7407 - mask_10_4_0_output_loss: 2.7406 - mask_10_4_1_output_loss: 2.7354 - mask_10_5_0_output_loss: 2.7440 - mask_10_5_1_output_loss: 2.7410 - mask_10_6_0_output_loss: 2.7451 - mask_10_6_1_output_loss: 2.7443 - mask_11_0_0_output_loss: 2.7440 - mask_11_0_1_output_loss: 2.7409 - mask_11_1_0_output_loss: 2.7439 - mask_11_1_1_output_loss: 2.7401 - mask_11_2_0_output_loss: 2.7403 - mask_11_2_1_output_loss: 2.7337 - mask_11_3_0_output_loss: 2.7487 - mask_11_3_1_output_loss: 2.7463 - mask_11_4_0_output_loss: 2.7508 - mask_11_4_1_output_loss: 2.7493 - mask_11_5_0_output_loss: 2.7435 - mask_11_5_1_output_loss: 2.7395 - mask_11_6_0_output_loss: 2.7445 - mask_11_6_1_output_loss: 2.7429 - mask_12_0_0_output_loss: 2.7387 - mask_12_0_1_output_loss: 2.7359 - mask_12_1_0_output_loss: 2.7499 - mask_12_1_1_output_loss: 2.7478 - mask_12_2_0_output_loss: 2.7478 - mask_12_2_1_output_loss: 2.7455 - mask_12_3_0_output_loss: 2.7444 - mask_12_3_1_output_loss: 2.7403 - mask_12_4_0_output_loss: 2.7402 - mask_12_4_1_output_loss: 2.7378 - mask_12_5_0_output_loss: 2.7462 - mask_12_5_1_output_loss: 2.7400 - mask_12_6_0_output_loss: 2.7464 - mask_12_6_1_output_loss: 2.7443 - mask_13_0_0_output_loss: 2.7433 - mask_13_0_1_output_loss: 2.7426 - mask_13_1_0_output_loss: 2.7433 - mask_13_1_1_output_loss: 2.7416 - mask_13_2_0_output_loss: 2.7399 - mask_13_2_1_output_loss: 2.7364 - mask_13_3_0_output_loss: 2.7480 - mask_13_3_1_output_loss: 2.7451 - mask_13_4_0_output_loss: 2.7492 - mask_13_4_1_output_loss: 2.7507 - mask_13_5_0_output_loss: 2.7424 - mask_13_5_1_output_loss: 2.7403 - mask_13_6_0_output_loss: 2.7442 - mask_13_6_1_output_loss: 2.7407 - rws_perm_output_accuracy: 0.9890 - rws_mask_0_0_output_accuracy: 0.0928 - rws_mask_0_1_output_accuracy: 0.0933 - rws_mask_1_0_output_accuracy: 0.0938 - rws_mask_1_1_output_accuracy: 0.0922 - rws_mask_2_0_output_accuracy: 0.0917 - rws_mask_2_1_output_accuracy: 0.0930 - rws_mask_3_0_output_accuracy: 0.0926 - rws_mask_3_1_output_accuracy: 0.0903 - rws_mask_4_0_output_accuracy: 0.0952 - rws_mask_4_1_output_accuracy: 0.0921 - rws_mask_5_0_output_accuracy: 0.0957 - rws_mask_5_1_output_accuracy: 0.0922 - rws_mask_6_0_output_accuracy: 0.0938 - rws_mask_6_1_output_accuracy: 0.0933 - rws_mask_7_0_output_accuracy: 0.0932 - rws_mask_7_1_output_accuracy: 0.0904 - rws_mask_8_0_output_accuracy: 0.0904 - rws_mask_8_1_output_accuracy: 0.0914 - rws_mask_9_0_output_accuracy: 0.0956 - rws_mask_9_1_output_accuracy: 0.0932 - rws_mask_10_0_output_accuracy: 0.0957 - rws_mask_10_1_output_accuracy: 0.0930 - rws_mask_11_0_output_accuracy: 0.0944 - rws_mask_11_1_output_accuracy: 0.0912 - rws_mask_12_0_output_accuracy: 0.0944 - rws_mask_12_1_output_accuracy: 0.0923 - rws_mask_13_0_output_accuracy: 0.0911 - rws_mask_13_1_output_accuracy: 0.0910 - rws_mask_14_0_output_accuracy: 0.0946 - rws_mask_14_1_output_accuracy: 0.0940 - rws_mask_15_0_output_accuracy: 0.0944 - rws_mask_15_1_output_accuracy: 0.0929 - rws_mask_16_0_output_accuracy: 0.0951 - rws_mask_16_1_output_accuracy: 0.0901 - rws_mask_17_0_output_accuracy: 0.0922 - rws_mask_17_1_output_accuracy: 0.0928 - rws_mask_18_0_output_accuracy: 0.0920 - rws_mask_18_1_output_accuracy: 0.0916 - rws_mask_19_0_output_accuracy: 0.0929 - rws_mask_19_1_output_accuracy: 0.0918 - rws_mask_20_0_output_accuracy: 0.0947 - rws_mask_20_1_output_accuracy: 0.0944 - rws_mask_21_0_output_accuracy: 0.0985 - rws_mask_21_1_output_accuracy: 0.0905 - rws_mask_22_0_output_accuracy: 0.0936 - rws_mask_22_1_output_accuracy: 0.0918 - rws_mask_23_0_output_accuracy: 0.0895 - rws_mask_23_1_output_accuracy: 0.0913 - rws_mask_24_0_output_accuracy: 0.0934 - rws_mask_24_1_output_accuracy: 0.0936 - rws_mask_25_0_output_accuracy: 0.0927 - rws_mask_25_1_output_accuracy: 0.0915 - rws_mask_26_0_output_accuracy: 0.0954 - rws_mask_26_1_output_accuracy: 0.0897 - rws_mask_27_0_output_accuracy: 0.0923 - rws_mask_27_1_output_accuracy: 0.0908 - rws_mask_28_0_output_accuracy: 0.0913 - rws_mask_28_1_output_accuracy: 0.0907 - rws_mask_29_0_output_accuracy: 0.0924 - rws_mask_29_1_output_accuracy: 0.0922 - rws_mask_30_0_output_accuracy: 0.0942 - rws_mask_30_1_output_accuracy: 0.0923 - rws_mask_31_0_output_accuracy: 0.0947 - rws_mask_31_1_output_accuracy: 0.0920 - rws_mask_32_0_output_accuracy: 0.0920 - rws_mask_32_1_output_accuracy: 0.0942 - rws_mask_33_0_output_accuracy: 0.0896 - rws_mask_33_1_output_accuracy: 0.0912 - rws_mask_34_0_output_accuracy: 0.0934 - rws_mask_34_1_output_accuracy: 0.0915 - rws_mask_35_0_output_accuracy: 0.0940 - rws_mask_35_1_output_accuracy: 0.0916 - rws_mask_36_0_output_accuracy: 0.0948 - rws_mask_36_1_output_accuracy: 0.0902 - rws_mask_37_0_output_accuracy: 0.0932 - rws_mask_37_1_output_accuracy: 0.0947 - rws_mask_38_0_output_accuracy: 0.0925 - rws_mask_38_1_output_accuracy: 0.0919 - rws_mask_39_0_output_accuracy: 0.0918 - rws_mask_39_1_output_accuracy: 0.0938 - rws_mask_40_0_output_accuracy: 0.0942 - rws_mask_40_1_output_accuracy: 0.0934 - rws_mask_41_0_output_accuracy: 0.0972 - rws_mask_41_1_output_accuracy: 0.0922 - rws_mask_42_0_output_accuracy: 0.0920 - rws_mask_42_1_output_accuracy: 0.0916 - rws_mask_43_0_output_accuracy: 0.0908 - rws_mask_43_1_output_accuracy: 0.0925 - rws_mask_44_0_output_accuracy: 0.0938 - rws_mask_44_1_output_accuracy: 0.0907 - rws_mask_45_0_output_accuracy: 0.0933 - rws_mask_45_1_output_accuracy: 0.0905 - rws_mask_46_0_output_accuracy: 0.0943 - rws_mask_46_1_output_accuracy: 0.0915 - rws_mask_47_0_output_accuracy: 0.0932 - rws_mask_47_1_output_accuracy: 0.0906 - rws_mask_48_0_output_accuracy: 0.0915 - rws_mask_48_1_output_accuracy: 0.0918 - rws_mask_49_0_output_accuracy: 0.0921 - rws_mask_49_1_output_accuracy: 0.0922 - rws_mask_50_0_output_accuracy: 0.0943 - rws_mask_50_1_output_accuracy: 0.0910 - rws_mask_51_0_output_accuracy: 0.0963 - rws_mask_51_1_output_accuracy: 0.0909 - rws_mask_52_0_output_accuracy: 0.0951 - rws_mask_52_1_output_accuracy: 0.0913 - rws_mask_53_0_output_accuracy: 0.0936 - rws_mask_53_1_output_accuracy: 0.0915 - rws_mask_54_0_output_accuracy: 0.0943 - rws_mask_54_1_output_accuracy: 0.0905 - rws_mask_55_0_output_accuracy: 0.0930 - rws_mask_55_1_output_accuracy: 0.0936 - rws_mask_56_0_output_accuracy: 0.0944 - rws_mask_56_1_output_accuracy: 0.0936 - rws_mask_57_0_output_accuracy: 0.0922 - rws_mask_57_1_output_accuracy: 0.0925 - rws_mask_58_0_output_accuracy: 0.0926 - rws_mask_58_1_output_accuracy: 0.0889 - rws_mask_59_0_output_accuracy: 0.0918 - rws_mask_59_1_output_accuracy: 0.0935 - rws_mask_60_0_output_accuracy: 0.0941 - rws_mask_60_1_output_accuracy: 0.0922 - rws_mask_61_0_output_accuracy: 0.0982 - rws_mask_61_1_output_accuracy: 0.0909 - rws_mask_62_0_output_accuracy: 0.0926 - rws_mask_62_1_output_accuracy: 0.0935 - rws_mask_63_0_output_accuracy: 0.0932 - rws_mask_63_1_output_accuracy: 0.0889 - rws_mask_64_0_output_accuracy: 0.0928 - rws_mask_64_1_output_accuracy: 0.0914 - rws_mask_65_0_output_accuracy: 0.0913 - rws_mask_65_1_output_accuracy: 0.0908 - rws_mask_66_0_output_accuracy: 0.0972 - rws_mask_66_1_output_accuracy: 0.0891 - rws_mask_67_0_output_accuracy: 0.0951 - rws_mask_67_1_output_accuracy: 0.0930 - rws_mask_68_0_output_accuracy: 0.0923 - rws_mask_68_1_output_accuracy: 0.0912 - rws_mask_69_0_output_accuracy: 0.0906 - rws_mask_69_1_output_accuracy: 0.0913 - rws_mask_70_0_output_accuracy: 0.0931 - rws_mask_70_1_output_accuracy: 0.0919 - rws_mask_71_0_output_accuracy: 0.0954 - rws_mask_71_1_output_accuracy: 0.0914 - rws_mask_72_0_output_accuracy: 0.0948 - rws_mask_72_1_output_accuracy: 0.0928 - rws_mask_73_0_output_accuracy: 0.0937 - rws_mask_73_1_output_accuracy: 0.0893 - rws_mask_74_0_output_accuracy: 0.0948 - rws_mask_74_1_output_accuracy: 0.0907 - rws_mask_75_0_output_accuracy: 0.0924 - rws_mask_75_1_output_accuracy: 0.0898 - rws_mask_76_0_output_accuracy: 0.0968 - rws_mask_76_1_output_accuracy: 0.0915 - rws_mask_77_0_output_accuracy: 0.0945 - rws_mask_77_1_output_accuracy: 0.0926 - rws_mask_78_0_output_accuracy: 0.0931 - rws_mask_78_1_output_accuracy: 0.0923 - rws_mask_79_0_output_accuracy: 0.0935 - rws_mask_79_1_output_accuracy: 0.0923 - rws_mask_80_0_output_accuracy: 0.0911 - rws_mask_80_1_output_accuracy: 0.0922 - rws_mask_81_0_output_accuracy: 0.0953 - rws_mask_81_1_output_accuracy: 0.0932 - rws_mask_82_0_output_accuracy: 0.0946 - rws_mask_82_1_output_accuracy: 0.0920 - rws_mask_83_0_output_accuracy: 0.0956 - rws_mask_83_1_output_accuracy: 0.0918 - rws_mask_84_0_output_accuracy: 0.0938 - rws_mask_84_1_output_accuracy: 0.0920 - rws_mask_85_0_output_accuracy: 0.0932 - rws_mask_85_1_output_accuracy: 0.0909 - rws_mask_86_0_output_accuracy: 0.0929 - rws_mask_86_1_output_accuracy: 0.0923 - rws_mask_87_0_output_accuracy: 0.0936 - rws_mask_87_1_output_accuracy: 0.0934 - rws_mask_88_0_output_accuracy: 0.0967 - rws_mask_88_1_output_accuracy: 0.0917 - rws_mask_89_0_output_accuracy: 0.0908 - rws_mask_89_1_output_accuracy: 0.0937 - rws_mask_90_0_output_accuracy: 0.0920 - rws_mask_90_1_output_accuracy: 0.0932 - rws_mask_91_0_output_accuracy: 0.0946 - rws_mask_91_1_output_accuracy: 0.0931 - rws_mask_92_0_output_accuracy: 0.0952 - rws_mask_92_1_output_accuracy: 0.0939 - rws_mask_93_0_output_accuracy: 0.0955 - rws_mask_93_1_output_accuracy: 0.0936 - rws_mask_94_0_output_accuracy: 0.0930 - rws_mask_94_1_output_accuracy: 0.0932 - rws_mask_95_0_output_accuracy: 0.0911 - rws_mask_95_1_output_accuracy: 0.0934 - rws_mask_96_0_output_accuracy: 0.0940 - rws_mask_96_1_output_accuracy: 0.0927 - rws_mask_97_0_output_accuracy: 0.1000 - rws_mask_97_1_output_accuracy: 0.0950 - round_perm_output_accuracy: 0.8349 - block_perm_0_output_accuracy: 0.4782 - block_perm_1_output_accuracy: 0.4406 - block_perm_2_output_accuracy: 0.4835 - block_perm_3_output_accuracy: 0.4379 - block_perm_4_output_accuracy: 0.4753 - block_perm_5_output_accuracy: 0.4380 - block_perm_6_output_accuracy: 0.4786 - block_perm_7_output_accuracy: 0.4343 - block_perm_8_output_accuracy: 0.4791 - block_perm_9_output_accuracy: 0.4384 - block_perm_10_output_accuracy: 0.4794 - block_perm_11_output_accuracy: 0.4385 - block_perm_12_output_accuracy: 0.4775 - block_perm_13_output_accuracy: 0.4398 - mask_0_0_0_output_accuracy: 0.0942 - mask_0_0_1_output_accuracy: 0.0945 - mask_0_1_0_output_accuracy: 0.0891 - mask_0_1_1_output_accuracy: 0.0893 - mask_0_2_0_output_accuracy: 0.0913 - mask_0_2_1_output_accuracy: 0.0933 - mask_0_3_0_output_accuracy: 0.0936 - mask_0_3_1_output_accuracy: 0.0924 - mask_0_4_0_output_accuracy: 0.0964 - mask_0_4_1_output_accuracy: 0.0998 - mask_0_5_0_output_accuracy: 0.0934 - mask_0_5_1_output_accuracy: 0.0957 - mask_0_6_0_output_accuracy: 0.0963 - mask_0_6_1_output_accuracy: 0.0921 - mask_1_0_0_output_accuracy: 0.0934 - mask_1_0_1_output_accuracy: 0.0971 - mask_1_1_0_output_accuracy: 0.0926 - mask_1_1_1_output_accuracy: 0.0940 - mask_1_2_0_output_accuracy: 0.0937 - mask_1_2_1_output_accuracy: 0.0955 - mask_1_3_0_output_accuracy: 0.0902 - mask_1_3_1_output_accuracy: 0.0914 - mask_1_4_0_output_accuracy: 0.0906 - mask_1_4_1_output_accuracy: 0.0926 - mask_1_5_0_output_accuracy: 0.0951 - mask_1_5_1_output_accuracy: 0.0953 - mask_1_6_0_output_accuracy: 0.0958 - mask_1_6_1_output_accuracy: 0.0953 - mask_2_0_0_output_accuracy: 0.0955 - mask_2_0_1_output_accuracy: 0.0976 - mask_2_1_0_output_accuracy: 0.0899 - mask_2_1_1_output_accuracy: 0.0899 - mask_2_2_0_output_accuracy: 0.0937 - mask_2_2_1_output_accuracy: 0.0933 - mask_2_3_0_output_accuracy: 0.0930 - mask_2_3_1_output_accuracy: 0.0931 - mask_2_4_0_output_accuracy: 0.0947 - mask_2_4_1_output_accuracy: 0.0995 - mask_2_5_0_output_accuracy: 0.0931 - mask_2_5_1_output_accuracy: 0.0971 - mask_2_6_0_output_accuracy: 0.0919 - mask_2_6_1_output_accuracy: 0.0966 - mask_3_0_0_output_accuracy: 0.0958 - mask_3_0_1_output_accuracy: 0.0945 - mask_3_1_0_output_accuracy: 0.0936 - mask_3_1_1_output_accuracy: 0.0959 - mask_3_2_0_output_accuracy: 0.0946 - mask_3_2_1_output_accuracy: 0.0964 - mask_3_3_0_output_accuracy: 0.0920 - mask_3_3_1_output_accuracy: 0.0933 - mask_3_4_0_output_accuracy: 0.0913 - mask_3_4_1_output_accuracy: 0.0916 - mask_3_5_0_output_accuracy: 0.0934 - mask_3_5_1_output_accuracy: 0.0979 - mask_3_6_0_output_accuracy: 0.0920 - mask_3_6_1_output_accuracy: 0.0932 - mask_4_0_0_output_accuracy: 0.0942 - mask_4_0_1_output_accuracy: 0.0969 - mask_4_1_0_output_accuracy: 0.0905 - mask_4_1_1_output_accuracy: 0.0916 - mask_4_2_0_output_accuracy: 0.0919 - mask_4_2_1_output_accuracy: 0.0926 - mask_4_3_0_output_accuracy: 0.0937 - mask_4_3_1_output_accuracy: 0.0949 - mask_4_4_0_output_accuracy: 0.0947 - mask_4_4_1_output_accuracy: 0.0941 - mask_4_5_0_output_accuracy: 0.0907 - mask_4_5_1_output_accuracy: 0.0946 - mask_4_6_0_output_accuracy: 0.0912 - mask_4_6_1_output_accuracy: 0.0913 - mask_5_0_0_output_accuracy: 0.0947 - mask_5_0_1_output_accuracy: 0.0942 - mask_5_1_0_output_accuracy: 0.0928 - mask_5_1_1_output_accuracy: 0.0961 - mask_5_2_0_output_accuracy: 0.0940 - mask_5_2_1_output_accuracy: 0.0958 - mask_5_3_0_output_accuracy: 0.0895 - mask_5_3_1_output_accuracy: 0.0930 - mask_5_4_0_output_accuracy: 0.0904 - mask_5_4_1_output_accuracy: 0.0900 - mask_5_5_0_output_accuracy: 0.0964 - mask_5_5_1_output_accuracy: 0.0960 - mask_5_6_0_output_accuracy: 0.0928 - mask_5_6_1_output_accuracy: 0.0944 - mask_6_0_0_output_accuracy: 0.0943 - mask_6_0_1_output_accuracy: 0.0988 - mask_6_1_0_output_accuracy: 0.0905 - mask_6_1_1_output_accuracy: 0.0918 - mask_6_2_0_output_accuracy: 0.0921 - mask_6_2_1_output_accuracy: 0.0921 - mask_6_3_0_output_accuracy: 0.0933 - mask_6_3_1_output_accuracy: 0.0931 - mask_6_4_0_output_accuracy: 0.0943 - mask_6_4_1_output_accuracy: 0.0969 - mask_6_5_0_output_accuracy: 0.0914 - mask_6_5_1_output_accuracy: 0.0955 - mask_6_6_0_output_accuracy: 0.0939 - mask_6_6_1_output_accuracy: 0.0939 - mask_7_0_0_output_accuracy: 0.0935 - mask_7_0_1_output_accuracy: 0.0952 - mask_7_1_0_output_accuracy: 0.0929 - mask_7_1_1_output_accuracy: 0.0957 - mask_7_2_0_output_accuracy: 0.0939 - mask_7_2_1_output_accuracy: 0.0980 - mask_7_3_0_output_accuracy: 0.0903 - mask_7_3_1_output_accuracy: 0.0952 - mask_7_4_0_output_accuracy: 0.0887 - mask_7_4_1_output_accuracy: 0.0921 - mask_7_5_0_output_accuracy: 0.0951 - mask_7_5_1_output_accuracy: 0.0947 - mask_7_6_0_output_accuracy: 0.0921 - mask_7_6_1_output_accuracy: 0.0923 - mask_8_0_0_output_accuracy: 0.0970 - mask_8_0_1_output_accuracy: 0.0980 - mask_8_1_0_output_accuracy: 0.0890 - mask_8_1_1_output_accuracy: 0.0896 - mask_8_2_0_output_accuracy: 0.0928 - mask_8_2_1_output_accuracy: 0.0929 - mask_8_3_0_output_accuracy: 0.0935 - mask_8_3_1_output_accuracy: 0.0942 - mask_8_4_0_output_accuracy: 0.0944 - mask_8_4_1_output_accuracy: 0.0986 - mask_8_5_0_output_accuracy: 0.0928 - mask_8_5_1_output_accuracy: 0.0925 - mask_8_6_0_output_accuracy: 0.0943 - mask_8_6_1_output_accuracy: 0.0923 - mask_9_0_0_output_accuracy: 0.0931 - mask_9_0_1_output_accuracy: 0.0935 - mask_9_1_0_output_accuracy: 0.0941 - mask_9_1_1_output_accuracy: 0.0948 - mask_9_2_0_output_accuracy: 0.0964 - mask_9_2_1_output_accuracy: 0.0951 - mask_9_3_0_output_accuracy: 0.0908 - mask_9_3_1_output_accuracy: 0.0891 - mask_9_4_0_output_accuracy: 0.0896 - mask_9_4_1_output_accuracy: 0.0887 - mask_9_5_0_output_accuracy: 0.0941 - mask_9_5_1_output_accuracy: 0.0956 - mask_9_6_0_output_accuracy: 0.0920 - mask_9_6_1_output_accuracy: 0.0959 - mask_10_0_0_output_accuracy: 0.0967 - mask_10_0_1_output_accuracy: 0.0987 - mask_10_1_0_output_accuracy: 0.0887 - mask_10_1_1_output_accuracy: 0.0903 - mask_10_2_0_output_accuracy: 0.0928 - mask_10_2_1_output_accuracy: 0.0912 - mask_10_3_0_output_accuracy: 0.0910 - mask_10_3_1_output_accuracy: 0.0942 - mask_10_4_0_output_accuracy: 0.0952 - mask_10_4_1_output_accuracy: 0.0995 - mask_10_5_0_output_accuracy: 0.0928 - mask_10_5_1_output_accuracy: 0.0942 - mask_10_6_0_output_accuracy: 0.0925 - mask_10_6_1_output_accuracy: 0.0915 - mask_11_0_0_output_accuracy: 0.0943 - mask_11_0_1_output_accuracy: 0.0943 - mask_11_1_0_output_accuracy: 0.0946 - mask_11_1_1_output_accuracy: 0.0957 - mask_11_2_0_output_accuracy: 0.0950 - mask_11_2_1_output_accuracy: 0.0972 - mask_11_3_0_output_accuracy: 0.0897 - mask_11_3_1_output_accuracy: 0.0926 - mask_11_4_0_output_accuracy: 0.0908 - mask_11_4_1_output_accuracy: 0.0896 - mask_11_5_0_output_accuracy: 0.0960 - mask_11_5_1_output_accuracy: 0.0959 - mask_11_6_0_output_accuracy: 0.0930 - mask_11_6_1_output_accuracy: 0.0955 - mask_12_0_0_output_accuracy: 0.0963 - mask_12_0_1_output_accuracy: 0.0981 - mask_12_1_0_output_accuracy: 0.0887 - mask_12_1_1_output_accuracy: 0.0908 - mask_12_2_0_output_accuracy: 0.0925 - mask_12_2_1_output_accuracy: 0.0944 - mask_12_3_0_output_accuracy: 0.0924 - mask_12_3_1_output_accuracy: 0.0951 - mask_12_4_0_output_accuracy: 0.0954 - mask_12_4_1_output_accuracy: 0.0966 - mask_12_5_0_output_accuracy: 0.0927 - mask_12_5_1_output_accuracy: 0.0949 - mask_12_6_0_output_accuracy: 0.0923 - mask_12_6_1_output_accuracy: 0.0945 - mask_13_0_0_output_accuracy: 0.0936 - mask_13_0_1_output_accuracy: 0.0940 - mask_13_1_0_output_accuracy: 0.0937 - mask_13_1_1_output_accuracy: 0.0935 - mask_13_2_0_output_accuracy: 0.0961 - mask_13_2_1_output_accuracy: 0.0977 - mask_13_3_0_output_accuracy: 0.0908 - mask_13_3_1_output_accuracy: 0.0901 - mask_13_4_0_output_accuracy: 0.0892 - mask_13_4_1_output_accuracy: 0.0879 - mask_13_5_0_output_accuracy: 0.0946 - mask_13_5_1_output_accuracy: 0.0952 - mask_13_6_0_output_accuracy: 0.0933 - mask_13_6_1_output_accuracy: 0.0981"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mresnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./resnet_models/resnet_500000_haar_2.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[1;32md:\\Pierugo\\protected_side_channel_attack\\deep_learning.py:551\u001b[0m, in \u001b[0;36mResNetSCA.train_model_generator\u001b[1;34m(self, X_y_gen_training, X_y_gen_val, save_file_name, patience)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;66;03m# Early stopping callback\u001b[39;00m\n\u001b[0;32m    549\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39mpatience, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m--> 551\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_y_gen_training\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_y_gen_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\n","File \u001b[1;32mc:\\Users\\iot-user\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[1;32mc:\\Users\\iot-user\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[1;32mc:\\Users\\iot-user\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[1;32mc:\\Users\\iot-user\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[1;32mc:\\Users\\iot-user\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\iot-user\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\iot-user\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[1;32mc:\\Users\\iot-user\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[1;32mc:\\Users\\iot-user\\miniconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["history = resnet.train_model_generator(train_gen, val_gen, \"./resnet_models/resnet_500000_haar_2.keras\", patience=50)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1411/1411\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step\n","x_round: 24419/45133 = 0.5410453548401392\n","block_perm_0: 39134/45133 = 0.8670817362018922\n","mask_0_0: 354/45133 = 0.007843484811556953\n","mask_0_1: 365/45133 = 0.00808720891587087\n","mask_0_2: 305/45133 = 0.006757804710522234\n","mask_0_3: 302/45133 = 0.006691334500254802\n","mask_0_4: 332/45133 = 0.0073560366029291205\n","mask_0_5: 305/45133 = 0.006757804710522234\n","mask_0_6: 276/45133 = 0.006115259344603727\n"]},{"data":{"text/plain":["array([1.3536543, 0.3749589, 5.503599 , 5.5072737, 5.477    , 5.5458245,\n","       5.490437 , 5.568437 , 5.656007 ], dtype=float32)"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["from tensorflow.keras.models import load_model\n","from tensorflow.keras.losses import categorical_crossentropy\n","from tensorflow.keras.utils import to_categorical\n","\n","y_predicted = load_model(\"./resnet_models/resnet_350000_52_epochs.keras\").predict(X_extraction)\n","for i, k in enumerate(y_extraction.keys()):\n","    print(f\"{k}: {np.count_nonzero(np.argmax(y_predicted[i], axis=1) == y_extraction[k])}/{y_extraction[k].shape[0]} = {np.count_nonzero(np.argmax(y_predicted[i], axis=1) == y_extraction[k]) / y_extraction[k].shape[0]}\")\n","\n","np.mean([categorical_crossentropy(to_categorical(y_extraction[k]), y_predicted[i]) for i, k in enumerate(y_extraction.keys())], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":2}
