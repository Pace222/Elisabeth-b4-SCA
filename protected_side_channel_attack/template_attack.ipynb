{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from typing import List\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_path = \"..\\\\acquisition\\\\50000_same_varying-mask_nullmask_maskshuffle\\\\carto_eB4-Rnd-3-WhiteningAndFullFilter-Same-And-Varying-Multiple-Defenses.mat\"\n",
    "key_path = \"..\\\\acquisition\\\\50000_same_varying-mask_nullmask_maskshuffle\\\\carto_eB4-Rnd-3-WhiteningAndFullFilter-Same-And-Varying-Multiple-Defenses.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = EntireTraceIterator(traces_path, key_path, nr_populations=2, nr_scenarios=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLIEST_ROUND = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_ALPHABET = list(range(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sost_masks_card_m_v_y_subsample(seeds_sub: np.ndarray, key_shares_sub: np.ndarray, traces_sub: np.ndarray):\n",
    "    assert seeds_sub.shape[0] == traces_sub.shape[0]\n",
    "    assert key_shares_sub.shape == (KEY_WIDTH_B4, NR_SHARES)\n",
    "    card_g = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), 1), dtype=np.int32)\n",
    "    m = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), traces_sub.shape[1]), dtype=np.float32)\n",
    "    v = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), traces_sub.shape[1]), dtype=np.float64)\n",
    "    y = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, traces_sub.shape[0]), dtype=int)\n",
    "\n",
    "    for seed, (trace_idx, trace) in zip(seeds_sub, enumerate(traces_sub)):\n",
    "        indices, whitening = chacha_random_b4(seed)\n",
    "\n",
    "        for round_idx in range(EARLIEST_ROUND, KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4):\n",
    "            for block_idx in range(BLOCK_WIDTH_B4):\n",
    "                keyround_index = round_idx * BLOCK_WIDTH_B4 + block_idx\n",
    "\n",
    "                shares = np.zeros(NR_SHARES, dtype=int)\n",
    "                for share_idx in range(shares.shape[0] - 1):\n",
    "                    shares[share_idx] = key_shares_sub[indices[keyround_index], share_idx]\n",
    "                shares[-1] = (key_shares_sub[indices[keyround_index], -1] + whitening[keyround_index]) % 16\n",
    "\n",
    "                for share_idx, share in enumerate(shares):\n",
    "                    card_g[round_idx, block_idx, share_idx, share, 0] += 1\n",
    "                    m[round_idx, block_idx, share_idx, share] += trace\n",
    "                    v[round_idx, block_idx, share_idx, share] += np.square(trace.astype(np.float64))\n",
    "\n",
    "                    y[round_idx, block_idx, share_idx, trace_idx] = share\n",
    "    \n",
    "    return card_g, m, v, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sost_combine_subsamples(card_g: np.ndarray, m: np.ndarray, v: np.ndarray) -> np.ndarray:\n",
    "    assert card_g.shape == (KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), 1)\n",
    "    assert m.ndim == 5 and m.shape[:4] == card_g.shape[:4]\n",
    "    assert m.shape == v.shape\n",
    "    f = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, m.shape[-1]), dtype=np.float32)\n",
    "\n",
    "    m /= card_g\n",
    "    v = (v - card_g * m * m) / (card_g - 1)\n",
    "\n",
    "    for i in range(len(KEY_ALPHABET)):\n",
    "        for j in range(i + 1, len(KEY_ALPHABET)):\n",
    "            num = m[:, :, :, i] - m[:, :, :, j]\n",
    "            den = np.sqrt(v[:, :, :, i] / card_g[:, :, :, i] + v[:, :, :, j] / card_g[:, :, :, j])\n",
    "            f += np.square(num / den)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_largest_values_separated_by_cycle(f: np.ndarray, n: int, cycle_length: int) -> np.ndarray:\n",
    "    interesting_points_per_index = np.zeros((f.shape[0], f.shape[1], f.shape[2], n), dtype=int)\n",
    "    sorted_strengths = np.argsort(f, axis=3)[:, :, :, ::-1]\n",
    "\n",
    "    for round_idx in range(EARLIEST_ROUND, sorted_strengths.shape[0]):\n",
    "        for block_idx in range(sorted_strengths.shape[1]):\n",
    "            for share_idx in range(sorted_strengths.shape[2]):\n",
    "                largest_indices = []\n",
    "                for ind in sorted_strengths[round_idx, block_idx, share_idx]:\n",
    "                    if len(largest_indices) < n:\n",
    "                        if all(abs(i - ind) >= cycle_length for i in largest_indices):\n",
    "                            largest_indices.append(ind)\n",
    "                    else:\n",
    "                        break\n",
    "                interesting_points_per_index[round_idx, block_idx, share_idx] = np.array(largest_indices)\n",
    "    \n",
    "    return interesting_points_per_index\n",
    "\n",
    "def n_largest_values(f: np.ndarray, n: int) -> np.ndarray:\n",
    "    interesting_points_per_index = np.apply_along_axis(np.argpartition, axis=3, arr=f, kth=-n)[:, :, :, -n:]\n",
    "    return interesting_points_per_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_key_count_best(classifications_scores: np.ndarray, target_seeds: np.ndarray):\n",
    "    classifications_best = np.argmax(classifications_scores, axis=-1)\n",
    "\n",
    "    classifications_per_key_nibble = np.zeros((KEY_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET)), dtype=int)\n",
    "    for i, seed in enumerate(target_seeds):\n",
    "        indices, whitening = chacha_random_b4(seed)\n",
    "        for keyround_index in range(KEYROUND_WIDTH_B4):\n",
    "            key_index = indices[keyround_index]\n",
    "            round_idx = keyround_index // BLOCK_WIDTH_B4\n",
    "            block_idx = keyround_index % BLOCK_WIDTH_B4\n",
    "\n",
    "            if round_idx >= EARLIEST_ROUND:\n",
    "                for share_idx in range(NR_SHARES - 1):\n",
    "                    classifications_per_key_nibble[key_index, share_idx, classifications_best[round_idx, block_idx, share_idx, i]] += 1\n",
    "                classifications_per_key_nibble[key_index, NR_SHARES - 1, (classifications_best[round_idx, block_idx, NR_SHARES - 1, i] - whitening[keyround_index]) % 16] += 1\n",
    "\n",
    "    recovered_key = np.argmax(classifications_per_key_nibble, axis=2) # Majority voting\n",
    "    return np.sum(recovered_key, axis=1) % 16\n",
    "\n",
    "def recover_key_sum_probs_rank(classifications_scores: np.ndarray, target_seeds: np.ndarray):\n",
    "    classifications_ranks = np.apply_along_axis(lambda a: np.searchsorted(np.sort(-a), -a), axis=-1, arr=classifications_scores)\n",
    "\n",
    "    classifications_per_key_nibble = np.zeros((KEY_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET)), dtype=int)\n",
    "    for i, seed in enumerate(target_seeds):\n",
    "        indices, whitening = chacha_random_b4(seed)\n",
    "        for keyround_index in range(KEYROUND_WIDTH_B4):\n",
    "            key_index = indices[keyround_index]\n",
    "            round_idx = keyround_index // BLOCK_WIDTH_B4\n",
    "            block_idx = keyround_index % BLOCK_WIDTH_B4\n",
    "\n",
    "            if round_idx >= EARLIEST_ROUND:\n",
    "                for share_idx in range(NR_SHARES - 1):\n",
    "                    classifications_per_key_nibble[key_index, share_idx] += classifications_ranks[round_idx, block_idx, share_idx, i]\n",
    "                classifications_per_key_nibble[key_index, NR_SHARES - 1] += np.roll(classifications_ranks[round_idx, block_idx, NR_SHARES - 1, i], -whitening[keyround_index])\n",
    "\n",
    "    recovered_key = np.argmin(classifications_per_key_nibble, axis=2)\n",
    "    return np.sum(recovered_key, axis=1) % 16\n",
    "\n",
    "def recover_key_multiply_probs(classifications_scores: np.ndarray, target_seeds: np.ndarray):\n",
    "    classifications_per_key_nibble = np.zeros((KEY_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET)), dtype=np.longdouble)\n",
    "    for i, seed in enumerate(target_seeds):\n",
    "        indices, whitening = chacha_random_b4(seed)\n",
    "        for keyround_index in range(KEYROUND_WIDTH_B4):\n",
    "            key_index = indices[keyround_index]\n",
    "            round_idx = keyround_index // BLOCK_WIDTH_B4\n",
    "            block_idx = keyround_index % BLOCK_WIDTH_B4\n",
    "\n",
    "            if round_idx >= EARLIEST_ROUND:\n",
    "                for share_idx in range(NR_SHARES - 1):\n",
    "                    classifications_per_key_nibble[key_index, share_idx] += classifications_scores[round_idx, block_idx, share_idx, i]\n",
    "                classifications_per_key_nibble[key_index, NR_SHARES - 1] += np.roll(classifications_scores[round_idx, block_idx, NR_SHARES - 1, i], -whitening[keyround_index])\n",
    "\n",
    "    recovered_key = np.argmax(classifications_per_key_nibble, axis=2)\n",
    "    return np.sum(recovered_key, axis=1) % 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m y \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seeds_sub, traces_sub, key, key_shares_sub \u001b[38;5;129;01min\u001b[39;00m data_loader((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m0\u001b[39m,)):\n\u001b[1;32m----> 7\u001b[0m     card_g_sub, m_sub, v_sub, y_sub \u001b[38;5;241m=\u001b[39m \u001b[43msost_card_m_v_y_subsample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseeds_sub\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraces_sub\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     card_g \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m card_g_sub\n\u001b[0;32m      9\u001b[0m     m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m m_sub\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36msost_card_m_v_y_subsample\u001b[1;34m(seeds_sub, key, traces_sub)\u001b[0m\n\u001b[0;32m     15\u001b[0m             card_g[round_idx, block_idx, real_k, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     16\u001b[0m             m[round_idx, block_idx, real_k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m trace\n\u001b[1;32m---> 17\u001b[0m             v[round_idx, block_idx, real_k] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msquare(\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m             y[round_idx, block_idx, i] \u001b[38;5;241m=\u001b[39m real_k\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m card_g, m, v, y\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "card_g = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), 1), dtype=np.int32)\n",
    "m = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), data_loader.trace_size), dtype=np.float32)\n",
    "v = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), data_loader.trace_size), dtype=np.float64)\n",
    "y = []\n",
    "\n",
    "for seeds_sub, traces_sub, key, key_shares_sub in data_loader((0, 1), (0,)):\n",
    "    card_g_sub, m_sub, v_sub, y_sub = sost_masks_card_m_v_y_subsample(seeds_sub[1][0], key_shares_sub[1][0], traces_sub[1][0])\n",
    "    card_g += card_g_sub\n",
    "    m += m_sub\n",
    "    v += v_sub\n",
    "    y.append(y_sub)\n",
    "\n",
    "with open(\"card_m_v_y_2_shares_masking.pic\", \"wb\") as w:\n",
    "    pic.dump((card_g, m, v, y), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate(y, axis=3)\n",
    "f = sost_combine_subsamples(card_g, m, v)\n",
    "\n",
    "with open(\"y_f_2_shares_masking.pic\", \"wb\") as w:\n",
    "    pic.dump((y, f), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError\n",
    "n_folds = 1\n",
    "train_sizes = [45_000]\n",
    "val_size_grid = [5_000]\n",
    "\n",
    "grid = {\"num_features\": [40]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError\n",
    "card_g = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), 1), dtype=np.int32)\n",
    "m = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), data_loader.trace_size), dtype=np.float32)\n",
    "v = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), data_loader.trace_size), dtype=np.float64)\n",
    "y = []\n",
    "\n",
    "for seeds_sub, traces_sub, key, key_shares_sub in data_loader((0, 1), (0,)):\n",
    "    rs = ShuffleSplit(n_splits=n_folds, test_size=val_size_grid[0], train_size=train_sizes[0], random_state=0)\n",
    "    for cv, (train_index, val_index) in enumerate(rs.split(traces_sub[1][0])):\n",
    "        traces_train, seeds_train = traces_sub[1][0][train_index], seeds_sub[1][0][train_index]\n",
    "        card_g_sub, m_sub, v_sub, y_sub = sost_masks_card_m_v_y_subsample(seeds_train, key_shares_sub[1][0], traces_train)\n",
    "        card_g += card_g_sub\n",
    "        m += m_sub\n",
    "        v += v_sub\n",
    "        y.append(y_sub)\n",
    "\n",
    "with open(\"card_m_v_y_2_shares_masking.pic\", \"wb\") as w:\n",
    "    pic.dump((card_g, m, v, y), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError\n",
    "best_accuracy = -1\n",
    "grid_results = np.zeros((len(grid[\"num_features\"]), len(train_sizes), len(val_size_grid), 1, n_folds), dtype=np.float64)\n",
    "for l, num_features in enumerate(grid[\"num_features\"]):\n",
    "    for ts, train_size in enumerate(train_sizes):\n",
    "        f_trainval_methods = f\n",
    "        y_trainval = y\n",
    "\n",
    "        for m, val_size in enumerate(val_size_grid):\n",
    "            for seeds_sub, traces_sub, key, key_shares_sub in data_loader((0, 1), (0,)):\n",
    "                rs = ShuffleSplit(n_splits=n_folds, test_size=val_size_grid[0], train_size=train_sizes[0], random_state=0)\n",
    "                for cv, (train_index, val_index) in enumerate(rs.split(traces_sub[1][0])):\n",
    "\n",
    "                    print(f\"CV {cv}\", end=\"\\r\")\n",
    "                    traces_train, traces_val = traces_trainval[ts][train_index], traces_trainval[ts][val_index]\n",
    "                    seeds_train, seeds_val = seeds_trainval[ts][train_index], seeds_trainval[ts][val_index]\n",
    "                    y = y_trainval[m, cv]\n",
    "\n",
    "                    for n, f_trainval in enumerate(f_trainval_methods):\n",
    "                        f = f_trainval[m, cv]\n",
    "\n",
    "                        interesting_points_per_index = n_largest_values(f, num_features)\n",
    "                        traces_reduced = traces_train[:, interesting_points_per_index].transpose(1, 2, 0, 3)\n",
    "                        traces_val_reduced = traces_val[:, interesting_points_per_index].transpose(1, 2, 0, 3)\n",
    "\n",
    "                        traces_reduced_per_key = [[[traces_reduced[round_idx, block_idx, np.repeat((y == k)[round_idx, block_idx, :, np.newaxis], traces_reduced.shape[-1], axis=-1)].reshape(-1, traces_reduced.shape[-1]) for k in range(len(KEY_ALPHABET))] for block_idx in range(traces_reduced.shape[1])] for round_idx in range(traces_reduced.shape[0])]\n",
    "\n",
    "                        start = time()\n",
    "                        template_means = np.array([[[np.mean(traces_reduced_per_key[round_idx][block_idx][k], axis=-2) for k in range(len(KEY_ALPHABET))] for block_idx in range(traces_reduced.shape[1])] for round_idx in range(traces_reduced.shape[0])])\n",
    "                        template_covariances = np.array([[[np.nan_to_num(np.cov(traces_reduced_per_key[round_idx][block_idx][k], rowvar=False), nan=0.0) for k in range(template_means.shape[2])] for block_idx in range(template_means.shape[1])] for round_idx in range(template_means.shape[0])])\n",
    "                        pdfs = np.array([[[multivariate_normal(template_means[round_idx, block_idx, k], template_covariances[round_idx, block_idx, k], allow_singular=True) for k in range(template_means.shape[2])] for block_idx in range(template_means.shape[1])] for round_idx in range(template_means.shape[0])])\n",
    "                        times[0, n, cv] = time() - start\n",
    "                        \n",
    "\n",
    "                        start = time()\n",
    "                        val_probas = np.array([[[[pdfs[round_idx, block_idx, key_guess].logpdf(traces_val_reduced[round_idx, block_idx, trace]) for key_guess in range(pdfs.shape[2])] for trace in range(traces_val_reduced.shape[2])] for block_idx in range(pdfs.shape[1])] for round_idx in range(pdfs.shape[0])])\n",
    "                        recovered_key = recover_key_multiply_probs(val_probas, seeds_val)\n",
    "                        times[1, n, cv] = time() - start\n",
    "\n",
    "                        grid_results[l, ts, m, n, cv] = np.count_nonzero(recovered_key == real_keys[0]) / KEY_WIDTH_B4\n",
    "                    \n",
    "                print(f\"SOST [num features: {num_features}, train size: {train_size}, val size: {val_size}]: {np.mean(grid_results[l, ts, m, 0]):#.4g} ± {np.std(grid_results[l, ts, m, 0]):#.4g} ({grid_results[l, ts, m, 0]}). Training in {np.mean(times[0, 0]):#.4g} ± {np.std(times[0, 0]):#.4g} seconds. Extracting in {np.mean(times[1, 0]):#.4g} ± {np.std(times[1, 0]):#.4g} seconds.\")\n",
    "                if np.mean(grid_results[l, ts, m, 0]) > best_accuracy:\n",
    "                    print(\"New best model found ! (Above)\")\n",
    "                    best_accuracy = np.mean(grid_results[l, ts, m, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(classifier: ClassifierMixin, traces_reduced: np.ndarray, y: np.ndarray, **kwargs):\n",
    "    models = np.empty(traces_reduced.shape[:2], dtype=object)\n",
    "    for round_idx in range(EARLIEST_ROUND, models.shape[0]):\n",
    "        for block_idx in range(models.shape[1]):\n",
    "            clf = classifier(**kwargs)\n",
    "            clf.fit(traces_reduced[round_idx, block_idx], y[round_idx, block_idx])\n",
    "            models[round_idx, block_idx] = clf\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "train_size = 100_000\n",
    "val_size_grid = [1_000, 2_000, 5_000]\n",
    "with open(\"trainval_test_split_f_y_sost.pic\", \"rb\") as r:\n",
    "    traces_trainval, traces_test, seeds_trainval, seeds_test, f_trainval, y_trainval = pic.load(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "train_size = 60_000\n",
    "val_size_grid = [750, 1_000, 1_500]\n",
    "\n",
    "with open(\"trainval_test_split_f_y_sost_2.pic\", \"rb\") as r:\n",
    "    traces_trainval, traces_test, seeds_trainval, seeds_test, f_trainval, y_trainval = pic.load(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "train_sizes = [60_000, 100_000]\n",
    "val_size_grid = [500, 750, 1_000]\n",
    "\n",
    "with open(\"trainval_test_split_f_y_sost_3.pic\", \"rb\") as r:\n",
    "    traces_trainval, traces_test, seeds_trainval, seeds_test, f_trainval_tot, y_trainval_tot = pic.load(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "train_sizes = [60_000, 100_000]\n",
    "val_size_grid = [1_500, 2_000, 2_500]\n",
    "\n",
    "with open(\"trainval_test_split_f_y_sost_4.pic\", \"rb\") as r:\n",
    "    traces_trainval, traces_test, seeds_trainval, seeds_test, f_trainval_tot, y_trainval_tot = pic.load(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_trainval, traces_test, seeds_trainval, seeds_test = train_test_split(traces_total[0], seeds_total[0], train_size=train_size + max(val_size_grid), random_state=0)\n",
    "f_trainval = np.zeros((len(val_size_grid), n_folds, KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, traces_trainval.shape[1]), dtype=np.float32)\n",
    "y_trainval = np.zeros((len(val_size_grid), n_folds, KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, train_size), dtype=np.int32)\n",
    "\n",
    "for m, val_size in enumerate(val_size_grid):\n",
    "    rs = ShuffleSplit(n_splits=n_folds, test_size=val_size, train_size=train_size, random_state=0)\n",
    "    for cv, (train_index, val_index) in enumerate(rs.split(traces_trainval)):\n",
    "        traces_train, seeds_train = traces_trainval[train_index], seeds_trainval[train_index]\n",
    "        f, y = sost_masks_card_m_v_y_subsample(seeds_train, real_keys[0], traces_train)\n",
    "        f_trainval[m, cv] = f\n",
    "        y_trainval[m, cv] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "train_sizes = [60_000, 100_000]\n",
    "val_size_grid = [500, 750, 1_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "train_sizes = [60_000, 100_000]\n",
    "val_size_grid = [1_500, 2_000, 2_500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_trainval, traces_test, seeds_trainval, seeds_test = [], [], [], []\n",
    "f_trainval_tot = np.zeros((len(train_sizes), 1, len(val_size_grid), n_folds, KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, traces_total[0].shape[1]), dtype=np.float32)\n",
    "y_trainval_tot = []\n",
    "for ts, train_size in enumerate(train_sizes):\n",
    "    y_trainval = np.zeros((len(val_size_grid), n_folds, KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, train_size), dtype=np.int32)\n",
    "\n",
    "    s = train_test_split(traces_total[0], seeds_total[0], train_size=train_size + max(val_size_grid), random_state=0)\n",
    "    traces_trainval.append(s[0]), traces_test.append(s[1]), seeds_trainval.append(s[2]), seeds_test.append(s[3])\n",
    "\n",
    "    for m, val_size in enumerate(val_size_grid):\n",
    "        rs = ShuffleSplit(n_splits=n_folds, test_size=val_size, train_size=train_size, random_state=0)\n",
    "        for cv, (train_index, val_index) in enumerate(rs.split(traces_trainval[ts])):\n",
    "            traces_train, seeds_train = traces_trainval[ts][train_index], seeds_trainval[ts][train_index]\n",
    "            f, y = sost_masks_card_m_v_y_subsample(seeds_train, real_keys[0], traces_train)\n",
    "            f_trainval_tot[ts, 0, m, cv] = f\n",
    "            y_trainval[m, cv] = y\n",
    "    y_trainval_tot.append(y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trainval_test_split_f_y_sost_4.pic\", \"wb\") as w:\n",
    "    pic.dump((traces_trainval, traces_test, seeds_trainval, seeds_test, f_trainval_tot, y_trainval_tot), w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOST [{'max_iter': 300, 'max_depth': None, 'min_samples_leaf': 6}, num features: 60, train size: 60000, val size: 1500]: 0.9949 ± 0.0009568 ([0.99609375 0.99414062 0.99414062 0.99609375 0.99414062]). Training in 436.8 ± 33.17 seconds.\n",
      "New best model found ! (Above)\n",
      "SOST [{'max_iter': 300, 'max_depth': None, 'min_samples_leaf': 6}, num features: 60, train size: 60000, val size: 2000]: 0.9992 ± 0.0009568 ([0.99804688 1.         1.         0.99804688 1.        ]). Training in 474.2 ± 11.23 seconds.\n",
      "New best model found ! (Above)\n",
      "SOST [{'max_iter': 300, 'max_depth': None, 'min_samples_leaf': 6}, num features: 60, train size: 60000, val size: 2500]: 1.000 ± 0.000 ([1. 1. 1. 1. 1.]). Training in 487.4 ± 8.411 seconds.\n",
      "New best model found ! (Above)\n",
      "SOST [{'max_iter': 300, 'max_depth': None, 'min_samples_leaf': 6}, num features: 60, train size: 100000, val size: 1500]: 0.9996 ± 0.0007813 ([1.         1.         1.         1.         0.99804688]). Training in 1236. ± 97.10 seconds.\n",
      "SOST [{'max_iter': 300, 'max_depth': None, 'min_samples_leaf': 6}, num features: 60, train size: 100000, val size: 2000]: 1.000 ± 0.000 ([1. 1. 1. 1. 1.]). Training in 1328. ± 20.75 seconds.\n",
      "SOST [{'max_iter': 300, 'max_depth': None, 'min_samples_leaf': 6}, num features: 60, train size: 100000, val size: 2500]: 1.000 ± 0.000 ([1. 1. 1. 1. 1.]). Training in 1374. ± 13.79 seconds.\n"
     ]
    }
   ],
   "source": [
    "params_grid = [\n",
    "    {\n",
    "        \"estimator\": HistGradientBoostingClassifier,\n",
    "        \"num_features\": [60],\n",
    "        \"model_params\": {\n",
    "            \"max_iter\": [300],\n",
    "            \"max_depth\": [None],\n",
    "            \"min_samples_leaf\": [6]\n",
    "        }\n",
    "    },\n",
    "    # {\n",
    "    #     \"estimator\": RandomForestClassifier,\n",
    "    #     \"num_features\": [60],\n",
    "    #     \"model_params\": {\n",
    "    #         \"n_estimators\": [300],\n",
    "    #         \"max_depth\": [None],\n",
    "    #         \"min_samples_leaf\": [6, 8, 10]\n",
    "    #     }\n",
    "    # },\n",
    "    # {\n",
    "    #     \"estimator\": LinearSVC,\n",
    "    #     \"num_features\": [60],\n",
    "    #     \"model_params\": {\n",
    "    #         \"loss\": [\"hinge\"],\n",
    "    #         \"C\": [1, 10],\n",
    "    #     }\n",
    "    # },\n",
    "    # {\n",
    "    #     \"estimator\": SVC,\n",
    "    #     \"num_features\": [20, 40, 60],\n",
    "    #     \"model_params\": {\n",
    "    #         \"decision_function_shape\": [\"ovr\"],\n",
    "    #         \"kernel\": [\"rbf\"],\n",
    "    #         \"C\": [0.1, 1, 10],\n",
    "    #         \"gamma\": [\"scale\", \"auto\"]\n",
    "    #     }\n",
    "    # },\n",
    "    # {\n",
    "    #     \"estimator\": SVC,\n",
    "    #     \"num_features\": [20, 40, 60],\n",
    "    #     \"model_params\": {\n",
    "    #         \"decision_function_shape\": [\"ovr\"],\n",
    "    #         \"kernel\": [\"poly\"],\n",
    "    #         \"C\": [0.1, 1, 10],\n",
    "    #         \"degree\": [3, 5, 7]\n",
    "    #     }\n",
    "    # },\n",
    "]\n",
    "\n",
    "best_params = {}\n",
    "best_accuracy = -1\n",
    "\n",
    "for grid in params_grid:\n",
    "    param_combinations = list(product(*grid[\"model_params\"].values()))\n",
    "    grid_results = np.zeros((len(param_combinations), len(grid[\"num_features\"]), len(train_sizes), len(val_size_grid), f_trainval_tot.shape[1], n_folds), dtype=np.float64)\n",
    "    for combination_index, params in enumerate(param_combinations):\n",
    "        model_params_dict = dict(zip(grid[\"model_params\"].keys(), params))\n",
    "        for l, num_features in enumerate(grid[\"num_features\"]):\n",
    "            for ts, train_size in enumerate(train_sizes):\n",
    "                f_trainval_methods = f_trainval_tot[ts]\n",
    "                y_trainval = y_trainval_tot[ts]\n",
    "\n",
    "                for m, val_size in enumerate(val_size_grid):\n",
    "                    rs = ShuffleSplit(n_splits=n_folds, test_size=val_size, train_size=train_size, random_state=0)\n",
    "                    times = np.zeros((f_trainval_methods.shape[0], n_folds), dtype=np.float64)\n",
    "                    for cv, (train_index, val_index) in enumerate(rs.split(traces_trainval[ts])):\n",
    "                        print(f\"CV {cv}\", end=\"\\r\")\n",
    "                        traces_train, traces_val = traces_trainval[ts][train_index], traces_trainval[ts][val_index]\n",
    "                        seeds_train, seeds_val = seeds_trainval[ts][train_index], seeds_trainval[ts][val_index]\n",
    "                        y = y_trainval[m, cv]\n",
    "\n",
    "                        for n, f_trainval in enumerate(f_trainval_methods):\n",
    "                            f = f_trainval[m, cv]\n",
    "\n",
    "                            interesting_points_per_index = n_largest_values(f, num_features)\n",
    "                            traces_reduced = traces_train[:, interesting_points_per_index].transpose(1, 2, 0, 3)\n",
    "                            traces_val_reduced = traces_val[:, interesting_points_per_index].transpose(1, 2, 0, 3)\n",
    "\n",
    "                            start = time()\n",
    "                            models = train_models(grid[\"estimator\"], traces_reduced, y, **model_params_dict, random_state=0)\n",
    "                            times[n, cv] = time() - start\n",
    "                            classifications_scores = np.array([[np.log(mod.predict_proba(traces_val_reduced[round_idx, block_idx])) if round_idx >= EARLIEST_ROUND else [0] * len(KEY_ALPHABET) for block_idx, mod in enumerate(row)] for round_idx, row in enumerate(models)])\n",
    "\n",
    "                            recovered_key = recover_key_multiply_probs(classifications_scores, seeds_val)\n",
    "\n",
    "                            grid_results[combination_index, l, ts, m, n, cv] = np.count_nonzero(recovered_key == real_keys[0]) / KEY_WIDTH_B4\n",
    "                        \n",
    "                    print(f\"SOST [{model_params_dict}, num features: {num_features}, train size: {train_size}, val size: {val_size}]: {np.mean(grid_results[combination_index, l, ts, m, 0]):#.4g} ± {np.std(grid_results[combination_index, l, ts, m, 0]):#.4g} ({grid_results[combination_index, l, ts, m, 0]}). Training in {np.mean(times[0]):#.4g} ± {np.std(times[0]):#.4g} seconds.\")\n",
    "                    if np.mean(grid_results[combination_index, l, ts, m, 0]) > best_accuracy:\n",
    "                        print(\"New best model found ! (Above)\")\n",
    "                        best_accuracy = np.mean(grid_results[combination_index, l, ts, m, 0])\n",
    "\n",
    "                    #print(f\" DOM [{model_params_dict}, num features: {num_features}, train size: {train_size}, val size: {val_size}]: {np.mean(grid_results[combination_index, l, ts, m, 1]):#.4g} ± {np.std(grid_results[combination_index, l, ts, m, 1]):#.4g} ({grid_results[combination_index, l, ts, m, 1]}). Training in {np.mean(times[1]):#.4g} ± {np.std(times[1]):#.4g} seconds.\")\n",
    "                    #if np.mean(grid_results[combination_index, l, ts, m, 1]) > best_accuracy:\n",
    "                    #    print(\"New best model found ! (Above)\")\n",
    "                    #    best_accuracy = np.mean(grid_results[combination_index, l, ts, m, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retraining on all available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'n_estimators': 200, 'max_depth': None, 'min_samples_leaf': 6, 'num_features': 40, 'test_size': 1500}\n",
    "with open(\"f_y_entire_train.pic\", \"rb\") as r:\n",
    "    f, y = pic.load(r)\n",
    "interesting_points_per_index = n_largest_values(f, best_params[\"num_features\"])\n",
    "traces_reduced = traces_trainval[:, interesting_points_per_index].transpose(1, 2, 0, 3)\n",
    "traces_test_reduced = traces_test[:, interesting_points_per_index].transpose(1, 2, 0, 3)\n",
    "\n",
    "random_forests = train_random_forests(traces_reduced, y, n_estimators=best_params[\"n_estimators\"], max_depth=best_params[\"max_depth\"], min_samples_leaf=best_params[\"min_samples_leaf\"], n_jobs=-1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'max_depth': None,\n",
       " 'min_samples_leaf': 6,\n",
       " 'num_features': 40,\n",
       " 'test_size': 1500}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, y = sost_masks_card_m_v_y_subsample(seeds_trainval, real_keys[0], traces_trainval)\n",
    "interesting_points_per_index = n_largest_values(f, best_params[\"num_features\"])\n",
    "traces_reduced = traces_trainval[:, interesting_points_per_index].transpose(1, 2, 0, 3)\n",
    "traces_test_reduced = traces_test[:, interesting_points_per_index].transpose(1, 2, 0, 3)\n",
    "\n",
    "random_forests = train_random_forests(traces_reduced, y, n_estimators=best_params[\"n_estimators\"], max_depth=best_params[\"max_depth\"], min_samples_leaf=best_params[\"min_samples_leaf\"], n_jobs=-1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"f_y_entire_train.pic\", \"wb\") as w:\n",
    "    pic.dump((f, y), w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train parameters: { {k: v for k, v in best_params.items() if k != 'test_size'} }\")\n",
    "for test_size in [1_000, 1_500, 2_500, 5_000]:\n",
    "    print(f\"Test size {test_size}\")\n",
    "    sub_tests = np.zeros((traces_test_reduced.shape[2] // test_size, test_size), dtype=int)\n",
    "    available_indices = np.arange(traces_test_reduced.shape[2])\n",
    "\n",
    "    np.random.seed(test_size)\n",
    "    for trace_idx in range(sub_tests.shape[0]):\n",
    "        sub_tests[trace_idx] = np.random.choice(available_indices, test_size, replace=False)\n",
    "        available_indices = np.setdiff1d(available_indices, sub_tests[trace_idx])\n",
    "        \n",
    "    accuracies = np.zeros((sub_tests.shape[0], 3), np.float64)\n",
    "    for trace_idx, sub_test in enumerate(sub_tests):\n",
    "        sub_traces, sub_seeds = traces_test_reduced[:, :, sub_test], seeds_test[sub_test]\n",
    "        classifications_scores = np.array([[rfc.predict_log_proba(sub_traces[round_idx, block_idx]) if round_idx >= EARLIEST_ROUND else [0] * len(KEY_ALPHABET) for block_idx, rfc in enumerate(rfs)] for round_idx, rfs in enumerate(random_forests)])\n",
    "\n",
    "        accuracies[trace_idx, 0] = np.count_nonzero(recover_key_count_best(classifications_scores, sub_seeds) == real_keys[0]) / KEY_WIDTH_B4\n",
    "        accuracies[trace_idx, 1] = np.count_nonzero(recover_key_sum_probs_rank(classifications_scores, sub_seeds) == real_keys[0]) / KEY_WIDTH_B4\n",
    "        accuracies[trace_idx, 2] = np.count_nonzero(recover_key_multiply_probs(classifications_scores, sub_seeds) == real_keys[0]) / KEY_WIDTH_B4\n",
    "    \n",
    "        print(f\"{trace_idx}/{len(sub_tests)}: {accuracies[trace_idx, 0]} -> {accuracies[trace_idx, 1]} -> {accuracies[trace_idx, 2]}\")\n",
    "    mean_acc = np.mean(accuracies, axis=0)\n",
    "    std_acc  = np.std(accuracies, axis=0)\n",
    "\n",
    "    print(f\"Test size [{test_size}]. Expected accuracy on unseen data: {mean_acc[0]} ± {std_acc[0]} -> {mean_acc[1]} ± {std_acc[1]} -> {mean_acc[2]} ± {std_acc[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_test_path_1 = \"..\\\\acquisition\\\\carto_eB4-Rnd-3-WhiteningAndFullFilter-4_test_keys\\\\carto_eB4-Rnd-3-WhiteningAndFullFilter-Alternating-Same-And-Varying-Seeds.mat\"\n",
    "key_test_path_1 = \"..\\\\acquisition\\\\carto_eB4-Rnd-3-WhiteningAndFullFilter-4_test_keys\\\\carto_eB4-Rnd-3-WhiteningAndFullFilter-Alternating-Same-And-Varying-Seeds.log\"\n",
    "traces_test_path_2 = \"..\\\\acquisition\\\\carto_eB4-Rnd-3-WhiteningAndFullFilter-12_test_keys\\\\carto_eB4-Rnd-3-WhiteningAndFullFilter.mat\"\n",
    "key_test_path_2 = \"..\\\\acquisition\\\\carto_eB4-Rnd-3-WhiteningAndFullFilter-12_test_keys\\\\carto_eB4-Rnd-3-WhiteningAndFullFilter.log\"\n",
    "\n",
    "seeds_test_1, traces_test_1, real_keys_1 = load_data(traces_test_path_1, key_test_path_1)\n",
    "seeds_test_2, traces_test_2, real_keys_2 = load_data(traces_test_path_2, key_test_path_2)\n",
    "\n",
    "seeds_test_multiple_keys, traces_test_multiple_keys, real_keys_multiple_keys = seeds_test_1 + seeds_test_2, traces_test_1 + traces_test_2, real_keys_1 + real_keys_2\n",
    "del seeds_test_1, seeds_test_2, traces_test_1, traces_test_2, real_keys_1, real_keys_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_test_reduced_multiple_keys = [traces_test[:, interesting_points_per_index].transpose(1, 2, 0, 3) for traces_test in traces_test_multiple_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train parameters: { {k: v for k, v in best_params.items() if k != 'test_size'} }\")\n",
    "for k, (seeds_test, traces_test_reduced, real_keys) in enumerate(zip(seeds_test_multiple_keys, traces_test_reduced_multiple_keys, real_keys_multiple_keys)):\n",
    "    print(f\"Key {k}\")\n",
    "    for test_size in [1_000, 1_500, 2_500]:\n",
    "        print(f\"Test size {test_size}\")\n",
    "        sub_tests = np.zeros((traces_test_reduced.shape[2] // test_size, test_size), dtype=int)\n",
    "        available_indices = np.arange(traces_test_reduced.shape[2])\n",
    "\n",
    "        np.random.seed(test_size)\n",
    "        for trace_idx in range(sub_tests.shape[0]):\n",
    "            sub_tests[trace_idx] = np.random.choice(available_indices, test_size, replace=False)\n",
    "            available_indices = np.setdiff1d(available_indices, sub_tests[trace_idx])\n",
    "            \n",
    "        accuracies = np.zeros((sub_tests.shape[0], 3), np.float64)\n",
    "        for trace_idx, sub_test in enumerate(sub_tests):\n",
    "            sub_traces, sub_seeds = traces_test_reduced[:, :, sub_test], seeds_test[sub_test]\n",
    "            classifications_scores = np.array([[rfc.predict_log_proba(sub_traces[round_idx, block_idx]) if round_idx >= EARLIEST_ROUND else [0] * len(KEY_ALPHABET) for block_idx, rfc in enumerate(rfs)] for round_idx, rfs in enumerate(random_forests)])\n",
    "\n",
    "            accuracies[trace_idx, 0] = np.count_nonzero(recover_key_count_best(classifications_scores, sub_seeds) == real_keys) / KEY_WIDTH_B4\n",
    "            accuracies[trace_idx, 1] = np.count_nonzero(recover_key_sum_probs_rank(classifications_scores, sub_seeds) == real_keys) / KEY_WIDTH_B4\n",
    "            accuracies[trace_idx, 2] = np.count_nonzero(recover_key_multiply_probs(classifications_scores, sub_seeds) == real_keys) / KEY_WIDTH_B4\n",
    "        \n",
    "            print(f\"{trace_idx}/{len(sub_tests)}: {accuracies[trace_idx, 0]} -> {accuracies[trace_idx, 1]} -> {accuracies[trace_idx, 2]}\")\n",
    "        mean_acc = np.mean(accuracies, axis=0)\n",
    "        std_acc  = np.std(accuracies, axis=0)\n",
    "\n",
    "        print(f\"Test size [{test_size}]. Expected accuracy on unseen data: {mean_acc[0]} ± {std_acc[0]} -> {mean_acc[1]} ± {std_acc[1]} -> {mean_acc[2]} ± {std_acc[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjQAAAGdCAYAAAAFcOm4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAxMklEQVR4nO3de1RVdd7H8c8R5KYCasGBERGzvOIlNSPLS/KIxmM1uZ4pI3UaJ5c92GT4qDlPqemUZZndHH2aphxXOlqrO5WKKF4KL6GMpsZkQ2Ep0KSAeANlP3+42HkUL9CBw++c92utvZZ779/Z+7t/+7DPx332PtthWZYlAAAAgzXxdAEAAAC/FIEGAAAYj0ADAACMR6ABAADGI9AAAADjEWgAAIDxCDQAAMB4BBoAAGA8f08XUF+qqqp08OBBtWjRQg6Hw9PlAACAK2BZlo4eParo6Gg1aXLl5128NtAcPHhQMTExni4DAADUwYEDB9SmTZsrbu+1gaZFixaSznZIaGioh6sBAABXoqysTDExMfbn+JXy2kBT/TVTaGgogQYAAMPU9nIRLgoGAADGI9AAAADjEWgAAIDxvPYaGgAAzmVZlk6fPq0zZ854uhSf5ufnJ39/f7f/pAqBBgDg9SoqKnTo0CEdP37c06VAUkhIiKKiohQQEOC2ZRJoAABeraqqSvn5+fLz81N0dLQCAgL4wVUPsSxLFRUV+vHHH5Wfn69rr722Vj+edykEGgCAV6uoqFBVVZViYmIUEhLi6XJ8XnBwsJo2barvvvtOFRUVCgoKcstyuSgYAOAT3HUmAL9cfewL9i4AADAegQYAABiPa2gAAD6r3aMfN9i6vn06uVbtFy9erClTpujIkSPy9z/7cV1eXq6WLVuqf//+ysrKsttmZWVp8ODB2r9/v6655hp3lm0MztAAANAIDR48WOXl5friiy/saZs2bZLT6dTWrVt18uRJe/r69evVtm3bC8JMRUVFg9XraQQaAAAaoY4dOyoqKuqCMzF33HGH4uLitGXLFpfpgwcP1m9/+1vdeeedevLJJxUdHa2OHTtKknbv3q1bb71VwcHBat26tcaPH6/y8nL79dWve+655xQVFaXWrVsrNTVVlZWVdptDhw4pOTlZwcHBiouL0/Lly9WuXTu98MIL9d4XV4JAAwBAIzV48GCtX7/eHl+/fr0GDRqkgQMH2tNPnDihrVu3avDgwZKkzMxM5eXlKSMjQ+np6Tp27JiSkpLUsmVLbd++XW+//bbWrl2riRMnuqxr/fr1+uabb7R+/Xr97W9/05IlS7RkyRJ7/pgxY3Tw4EFlZWXpnXfe0auvvqri4uL674QrxDU0QCNQ0/f4tf2+HYD3GTx4sCZNmqTTp0/rxIkT2rlzpwYOHKjKykotXrxYkpSdna1Tp07Z4adZs2Z67bXX7F/h/ctf/qKTJ09q6dKlatasmSTplVde0YgRI/TMM88oMjJSktSyZUu98sor8vPzU6dOnZScnKzMzEw98MAD+uqrr7R27Vpt375dffr0kSS99tpruvbaaz3QKzXjDA0AAI3UoEGDdOzYMW3fvl2bNm3Sddddp6uvvloDBw60r6PJyspS+/bt1bZtW0lSfHy8yyMF9u3bpx49ethhRpL69++vqqoq5eXl2dO6du0qPz8/ezwqKso+A5OXlyd/f39df/319vwOHTqoZcuW9bbttcUZGgAAGqkOHTqoTZs2Wr9+vY4cOaKBAwdKkqKjoxUTE6PPP/9c69ev16233mq/5tzgUhtNmzZ1GXc4HKqqqqp78Q2MQAPjnf91DV/VAN7Dm/6+d31fYv+7e5vwK37d4MGDlZWVpSNHjmjKlCn29AEDBujTTz/Vtm3b9OCDD1709Z07d9aSJUt07NgxO+x89tlnatKkiX3R8OV07NhRp0+f1s6dO9W7d29J0v79+3XkyJEr3o76RqAB4LW86cOwtrguy3sMHjzYvuOo+gyNJA0cOFATJ05URUWFfUFwTVJSUjRz5kyNHTtWs2bN0o8//qiHHnpIo0ePtq+fuZxOnTopMTFR48eP16JFi9S0aVNNnjxZwcHBjeZBnwQawMv58od6XRAE0NgMHjxYJ06cUKdOnVwCyMCBA3X06FH79u6LCQkJ0erVq/Xwww+rb9++CgkJ0ciRI/X888/Xqo6lS5dq3LhxGjBggJxOp+bOnas9e/a47eGSvxSBpg444F0eH6K+if0O0zTEe/Tcr5rqol27drIs64LpsbGxF0w/9zbrc8XHx2vdunUXXUdNrzv/92WioqL0ySef2OPff/+9iouL1aFDh4sX34AINF6AgAUA3uf8IFSb627qY11/WfmhThwrV4dOXfXv4kK9+twctWvXTgMGDKi3umqDQOPDCEJoDHgfNi6+fJatIQOEiU5XVuqlZ+boh4LvFNK8uQbc3F/Lli274O4oTyHQAACAy+o/aIj6Dxpijze2wEeggU+qz7MCvvw/XNQOZ6cA9yHQ1BMOVADwM46JqG88+gAAABiPQAMAAIxHoAEAAMYj0AAAAOMRaAAAgPG4ywkA4Ltmhdn/7F5Pq6he7q7ff1en1/+7uEh/eXm+tm5Yqx9++EERERHq2bOnJk2apCFDhlx+AQ1oyZIlmjRpkkpKShp83QQaAAAaqR8OFGjsr4epRViYnn32WcXHx6uyslKrV69Wamqqvvrqq1ovs6KiQgEBARdMr6ysbDS/+lsXfOUEAEAj9dT/TpbD4dCyj9Zq5MiRuu6669S1a1elpaVpy5YtkqSCggLdcccdat68uUJDQ/Wb3/xGRUVF9jJmzZqlnj176rXXXlNcXJz9dGyHw6FFixbp9ttvV7/rfqXXXp4vSVq/+hNdf/31CgoKUvv27fXEE0/o9OnT9vLKSks1+9FJioyMVFBQkLp166b09HRlZWXp/vvvV2lpqRwOhxwOh2bNmtVgfcUZGgAAGqHSI0f0WVamHpr6mEJCml0wPzw8XFVVVXaY2bBhg06fPq3U1FTdfffdysrKstvu379f77zzjt599135+fnZ02fNmqWnn35aE6bNlp+/n3Zs/VyPPTJBr7z8sm655RZ98803Gj9+vCTp1+MeVlVVlVLH/JeOlR/Vm2++qWuuuUZ79+6Vn5+fbrrpJr3wwguaMWOG8vLyJEnNmzev3046B4EGAIBGqODbf8myLLW75rqLtsnMzNTu3buVn5+vmJgYSdLSpUvVtWtXbd++XX379pV09mumpUuX6uqrr3Z5/b333qv777/ffjDnzP95SL/770kaO3asJKl9+/aaM2eOpk6dql+Pe1hbNmXpy9wcvbd+q/5jQB+7TbWwsDA5HA45nU53dcMVI9AAANAIWbIu22bfvn2KiYmxw4wkdenSReHh4dq3b58daGJjYy8IM5LUp08fl/F/7v1Sudu36q+vPG9PO3PmjE6ePKkTJ44rb+9uRUZFq137DnXdrHpDoAEAoBGKbXeNHA6Hvv3mn794Wc2aXfiVVU3Tjx87pgcnP6rU+1MuaFseEKTAoOBfXEt94aJgAAAaobCWLXXTwFu14m9/1fHjxy6YX1JSos6dO+vAgQM6cOCAPX3v3r0qKSlRly5dar3OzvHd9e03+9WhQ4cLhiZNmui6Tl1VdOigvv3X/hpfHxAQoDNnztR6ve5AoAEAoJH645+eU1XVGaWMSNQ777yjr7/+Wvv27dNLL72khIQEJSYmKj4+XikpKdqxY4e2bdumMWPGaODAgRd8nXQlxj88VenvrNATTzyhPXv2aN++fVqxYoUee+wxSVKfhP66vt9Nmjx+jDIyMpSfn69PP/1Uq1atkiS1a9dO5eXlyszM1L///W8dP37crf1xKQQaAAAaqTax7bTikyz1TbhZkydPVrdu3fQf//EfyszM1KJFi+RwOPTBBx+oZcuWGjBggBITE9W+fXutXLmyTuvrP2iIXnpjhdasWaO+ffvqxhtv1IIFCxQbG2u3ef7/lqprj+s1atQodenSRVOnTrXPytx0002aMGGC7r77bl199dWaN2+eW/rhSnANDQDAd80qtf9ZfadPte5twt2yivOXW1tXRzr1xz89e9F62rZtqw8++OCir581a1aNvwdjWTVfdNx/0BA9eN/IC6ZXb0dYy5aaPf+Vi9azaNEiLVq06KL11BfO0AAAAOMRaAAAgPEINAAAwHgEGgAAYDwCDQAAMB6BBgDgEy52Vw8aXn3si1oFmrlz56pv375q0aKFIiIidOedd9pP1Kw2aNAg+7Hh1cOECRNc2hQUFCg5OVkhISGKiIjQlClTXB5NLklZWVm6/vrrFRgYqA4dOmjJkiV120IAgE9r2rSpJDXoj7zh0qr3RfW+cYda/Q7Nhg0blJqaqr59++r06dP64x//qKFDh2rv3r0uz4N44IEHNHv2bHs8JCTE/veZM2eUnJwsp9Opzz//XIcOHdKYMWPUtGlTPfXUU5Kk/Px8JScna8KECVq2bJkyMzP1+9//XlFRUUpKSvql2wwA8CF+fn4KDw9XcXGxpLOfSQ6H44J21ukKl/GTJ0+6Zf3nL/dKl11f9dR1Xe6ox7IsHT9+XMXFxQoPD5efn1+tl3ExtQo01T9tXG3JkiWKiIhQTk6OBgwYYE8PCQm56KPD16xZo71792rt2rWKjIxUz549NWfOHE2bNk2zZs1SQECAFi9erLi4OM2fP1+S1LlzZ23evFkLFiwg0AAAaq36M6k61NSk+MgJl/GAE+55EOP5y73SZddXPXVdlzvrCQ8Pv2hOqKtf9EvBpaVnf2GxVatWLtOXLVumN998U06nUyNGjNDjjz9un6XJzs5WfHy8IiMj7fZJSUl68MEHtWfPHvXq1UvZ2dlKTEx0WWZSUpImTZp00VpOnTqlU6dO2eNlZWW/ZNMAAF7E4XAoKipKERERqqysrLHN79/NchnPnDzILes+f7lXuuz6qqeu63JXPU2bNnXrmZlqdQ40VVVVmjRpkvr3769u3brZ0++9917FxsYqOjpau3bt0rRp05SXl6d3331XklRYWOgSZiTZ44WFhZdsU1ZWphMnTig4+MJUOHfuXD3xxBN13RwAgA/w8/O76IfpD0ddnxIdFBTklnWev9wrXXZ91VPXdTVkPXVR50CTmpqqL7/8Ups3b3aZPn78ePvf8fHxioqK0pAhQ/TNN9/ommuuqXullzF9+nSlpaXZ42VlZYqJiam39QEAgMajTrdtT5w4Uenp6Vq/fr3atGlzybb9+vWTJO3fv1/S2e8xi4qKXNpUj1d/n3axNqGhoTWenZGkwMBAhYaGugwAAMA31CrQWJaliRMn6r333tO6desUFxd32dfk5uZKkqKioiRJCQkJ2r17t8uFWRkZGQoNDVWXLl3sNpmZmS7LycjIUEJCQm3KBQAAPqJWgSY1NVVvvvmmli9frhYtWqiwsFCFhYU6ceLslc/ffPON5syZo5ycHH377bf68MMPNWbMGA0YMEDdu3eXJA0dOlRdunTR6NGj9Y9//EOrV6/WY489ptTUVAUGBkqSJkyYoH/961+aOnWqvvrqK/35z3/WW2+9pUceecTNmw8AALxBrQLNokWLVFpaqkGDBikqKsoeVq5cKUkKCAjQ2rVrNXToUHXq1EmTJ0/WyJEj9dFHH9nL8PPzU3p6uvz8/JSQkKD77rtPY8aMcfndmri4OH388cfKyMhQjx49NH/+fL322mvcsg0AAGpUq4uCL/dTxTExMdqwYcNllxMbG6tPPvnkkm0GDRqknTt31qY8AADgo3iWEwAAMB6BBgAAGI9AAwAAjEegAQAAxiPQAAAA4xFoAACA8Qg0AADAeAQaAABgPAINAAAwHoEGAAAYj0ADAACMR6ABAADGI9AAAADjEWgAAIDxCDQAAMB4BBoAAGA8Ag0AADAegQYAABiPQAMAAIxHoAEAAMYj0AAAAOMRaAAAgPEINAAAwHgEGgAAYDwCDQAAMB6BBgAAGI9AAwAAjEegAQAAxiPQAAAA4xFoAACA8Qg0AADAeAQaAABgPAINAAAwHoEGAAAYj0ADAACMR6ABAADGI9AAAADjEWgAAIDxCDQAAMB4BBoAAGA8Ag0AADAegQYAABiPQAMAAIxHoAEAAMYj0AAAAOMRaAAAgPEINAAAwHgEGgAAYDwCDQAAMB6BBgAAGI9AAwAAjEegAQAAxqtVoJk7d6769u2rFi1aKCIiQnfeeafy8vJc2pw8eVKpqalq3bq1mjdvrpEjR6qoqMilTUFBgZKTkxUSEqKIiAhNmTJFp0+fdmmTlZWl66+/XoGBgerQoYOWLFlSty0EAABer1aBZsOGDUpNTdWWLVuUkZGhyspKDR06VMeOHbPbPPLII/roo4/09ttva8OGDTp48KDuuusue/6ZM2eUnJysiooKff755/rb3/6mJUuWaMaMGXab/Px8JScna/DgwcrNzdWkSZP0+9//XqtXr3bDJgMAAG/jX5vGq1atchlfsmSJIiIilJOTowEDBqi0tFR//etftXz5ct16662SpDfeeEOdO3fWli1bdOONN2rNmjXau3ev1q5dq8jISPXs2VNz5szRtGnTNGvWLAUEBGjx4sWKi4vT/PnzJUmdO3fW5s2btWDBAiUlJblp0wEAgLf4RdfQlJaWSpJatWolScrJyVFlZaUSExPtNp06dVLbtm2VnZ0tScrOzlZ8fLwiIyPtNklJSSorK9OePXvsNucuo7pN9TJqcurUKZWVlbkMAADAN9Q50FRVVWnSpEnq37+/unXrJkkqLCxUQECAwsPDXdpGRkaqsLDQbnNumKmeXz3vUm3Kysp04sSJGuuZO3euwsLC7CEmJqaumwYAAAxT50CTmpqqL7/8UitWrHBnPXU2ffp0lZaW2sOBAwc8XRIAAGggtbqGptrEiROVnp6ujRs3qk2bNvZ0p9OpiooKlZSUuJylKSoqktPptNts27bNZXnVd0Gd2+b8O6OKiooUGhqq4ODgGmsKDAxUYGBgXTYHAAAYrlZnaCzL0sSJE/Xee+9p3bp1iouLc5nfu3dvNW3aVJmZmfa0vLw8FRQUKCEhQZKUkJCg3bt3q7i42G6TkZGh0NBQdenSxW5z7jKq21QvAwAA4Fy1OkOTmpqq5cuX64MPPlCLFi3sa17CwsIUHByssLAwjRs3TmlpaWrVqpVCQ0P10EMPKSEhQTfeeKMkaejQoerSpYtGjx6tefPmqbCwUI899phSU1PtMywTJkzQK6+8oqlTp+p3v/ud1q1bp7feeksff/yxmzcfAAB4g1qdoVm0aJFKS0s1aNAgRUVF2cPKlSvtNgsWLNB//ud/auTIkRowYICcTqfeffdde76fn5/S09Pl5+enhIQE3XfffRozZoxmz55tt4mLi9PHH3+sjIwM9ejRQ/Pnz9drr73GLdsAAKBGtTpDY1nWZdsEBQVp4cKFWrhw4UXbxMbG6pNPPrnkcgYNGqSdO3fWpjwAAOCjeJYTAAAwHoEGAAAYj0ADAACMR6ABAADGI9AAAADjEWgAAIDxCDQAAMB4BBoAAGA8Ag0AADAegQYAABiPQAMAAIxHoAEAAMYj0AAAAOMRaAAAgPEINAAAwHgEGgAAYDwCDQAAMB6BBgAAGI9AAwAAjEegAQAAxiPQAAAA4xFoAACA8Qg0AADAeAQaAABgPAINAAAwHoEGAAAYj0ADAACMR6ABAADGI9AAAADjEWgAAIDxCDQAAMB4BBoAAGA8Ag0AADAegQYAABiPQAMAAIxHoAEAAMYj0AAAAOMRaAAAgPEINAAAwHgEGgAAYDwCDQAAMB6BBgAAGI9AAwAAjEegAQAAxiPQAAAA4xFoAACA8Qg0AADAeAQaAABgPAINAAAwHoEGAAAYj0ADAACMR6ABAADGq3Wg2bhxo0aMGKHo6Gg5HA69//77LvN/+9vfyuFwuAzDhg1zaXP48GGlpKQoNDRU4eHhGjdunMrLy13a7Nq1S7fccouCgoIUExOjefPm1X7rAACAT6h1oDl27Jh69OihhQsXXrTNsGHDdOjQIXv4+9//7jI/JSVFe/bsUUZGhtLT07Vx40aNHz/enl9WVqahQ4cqNjZWOTk5evbZZzVr1iy9+uqrtS0XAAD4AP/avmD48OEaPnz4JdsEBgbK6XTWOG/fvn1atWqVtm/frj59+kiSXn75Zd1222167rnnFB0drWXLlqmiokKvv/66AgIC1LVrV+Xm5ur55593CT4AAABSPV1Dk5WVpYiICHXs2FEPPvigfvrpJ3tedna2wsPD7TAjSYmJiWrSpIm2bt1qtxkwYIACAgLsNklJScrLy9ORI0dqXOepU6dUVlbmMgAAAN/g9kAzbNgwLV26VJmZmXrmmWe0YcMGDR8+XGfOnJEkFRYWKiIiwuU1/v7+atWqlQoLC+02kZGRLm2qx6vbnG/u3LkKCwuzh5iYGHdvGgAAaKRq/ZXT5dxzzz32v+Pj49W9e3ddc801ysrK0pAhQ9y9Otv06dOVlpZmj5eVlRFqAADwEfV+23b79u111VVXaf/+/ZIkp9Op4uJilzanT5/W4cOH7etunE6nioqKXNpUj1/s2pzAwECFhoa6DAAAwDfUe6D5/vvv9dNPPykqKkqSlJCQoJKSEuXk5Nht1q1bp6qqKvXr189us3HjRlVWVtptMjIy1LFjR7Vs2bK+SwYAAIapdaApLy9Xbm6ucnNzJUn5+fnKzc1VQUGBysvLNWXKFG3ZskXffvutMjMzdccdd6hDhw5KSkqSJHXu3FnDhg3TAw88oG3btumzzz7TxIkTdc899yg6OlqSdO+99yogIEDjxo3Tnj17tHLlSr344osuXykBAABUq3Wg+eKLL9SrVy/16tVLkpSWlqZevXppxowZ8vPz065du3T77bfruuuu07hx49S7d29t2rRJgYGB9jKWLVumTp06aciQIbrtttt08803u/zGTFhYmNasWaP8/Hz17t1bkydP1owZM7hlGwAA1KjWFwUPGjRIlmVddP7q1asvu4xWrVpp+fLll2zTvXt3bdq0qbblAQAAH8SznAAAgPEINAAAwHgEGgAAYDwCDQAAMB6BBgAAGM/tjz6A92v36Mcu498+nezR5bhLY6sHQOPDcaLxItB40Pl/GJJv/3HUZ3/UtGw0XvX1odGQf3PuWhd/F76hIfeFt+53Ag2AC5gYtj19kPb0+utLQwaqxhZcvXWfeisCTQPyZAKvzw8j/ugvzYRwwP8OG4/67B9f3s/e8FWRCccSTyLQuIkvHyjgOd5wkAY8wdPHUT4z3I9AgwbhrX9QjW27qKdxoz8ujf5pPEzcFwQaeB0T/xABAL8MgcZH8CEPAPBmBBrAAzwZMAm3ALwRgQaATyPgAd6BRx8AAADjEWgAAIDxCDQAAMB4BBoAAGA8Ag0AADAegQYAABiPQAMAAIxHoAEAAMYj0AAAAOMRaAAAgPEINAAAwHgEGgAAYDwCDQAAMB6BBgAAGI9AAwAAjEegAQAAxiPQAAAA4xFoAACA8Qg0AADAeAQaAABgPAINAAAwHoEGAAAYj0ADAACMR6ABAADGI9AAAADjEWgAAIDxCDQAAMB4BBoAAGA8Ag0AADAegQYAABiPQAMAAIxHoAEAAMYj0AAAAOMRaAAAgPEINAAAwHgEGgAAYDwCDQAAMF6tA83GjRs1YsQIRUdHy+Fw6P3333eZb1mWZsyYoaioKAUHBysxMVFff/21S5vDhw8rJSVFoaGhCg8P17hx41ReXu7SZteuXbrlllsUFBSkmJgYzZs3r/ZbBwAAfEKtA82xY8fUo0cPLVy4sMb58+bN00svvaTFixdr69atatasmZKSknTy5Em7TUpKivbs2aOMjAylp6dr48aNGj9+vD2/rKxMQ4cOVWxsrHJycvTss89q1qxZevXVV+uwiQAAwNv51/YFw4cP1/Dhw2ucZ1mWXnjhBT322GO64447JElLly5VZGSk3n//fd1zzz3at2+fVq1ape3bt6tPnz6SpJdfflm33XabnnvuOUVHR2vZsmWqqKjQ66+/roCAAHXt2lW5ubl6/vnnXYIPAACA5OZraPLz81VYWKjExER7WlhYmPr166fs7GxJUnZ2tsLDw+0wI0mJiYlq0qSJtm7darcZMGCAAgIC7DZJSUnKy8vTkSNHalz3qVOnVFZW5jIAAADf4NZAU1hYKEmKjIx0mR4ZGWnPKywsVEREhMt8f39/tWrVyqVNTcs4dx3nmzt3rsLCwuwhJibml28QAAAwgtfc5TR9+nSVlpbaw4EDBzxdEgAAaCBuDTROp1OSVFRU5DK9qKjInud0OlVcXOwy//Tp0zp8+LBLm5qWce46zhcYGKjQ0FCXAQAA+Aa3Bpq4uDg5nU5lZmba08rKyrR161YlJCRIkhISElRSUqKcnBy7zbp161RVVaV+/frZbTZu3KjKykq7TUZGhjp27KiWLVu6s2QAAOAFah1oysvLlZubq9zcXElnLwTOzc1VQUGBHA6HJk2apD/96U/68MMPtXv3bo0ZM0bR0dG68847JUmdO3fWsGHD9MADD2jbtm367LPPNHHiRN1zzz2Kjo6WJN17770KCAjQuHHjtGfPHq1cuVIvvvii0tLS3LbhAADAe9T6tu0vvvhCgwcPtserQ8bYsWO1ZMkSTZ06VceOHdP48eNVUlKim2++WatWrVJQUJD9mmXLlmnixIkaMmSImjRpopEjR+qll16y54eFhWnNmjVKTU1V7969ddVVV2nGjBncsg0AAGpU60AzaNAgWZZ10fkOh0OzZ8/W7NmzL9qmVatWWr58+SXX0717d23atKm25QEAAB/kNXc5AQAA30WgAQAAxiPQAAAA4xFoAACA8Qg0AADAeAQaAABgPAINAAAwHoEGAAAYj0ADAACMR6ABAADGI9AAAADjEWgAAIDxCDQAAMB4BBoAAGA8Ag0AADAegQYAABiPQAMAAIxHoAEAAMYj0AAAAOMRaAAAgPEINAAAwHgEGgAAYDwCDQAAMB6BBgAAGI9AAwAAjEegAQAAxiPQAAAA4xFoAACA8Qg0AADAeAQaAABgPAINAAAwHoEGAAAYj0ADAACMR6ABAADGI9AAAADjEWgAAIDxCDQAAMB4BBoAAGA8Ag0AADAegQYAABiPQAMAAIxHoAEAAMYj0AAAAOMRaAAAgPEINAAAwHgEGgAAYDwCDQAAMB6BBgAAGI9AAwAAjEegAQAAxiPQAAAA47k90MyaNUsOh8Nl6NSpkz3/5MmTSk1NVevWrdW8eXONHDlSRUVFLssoKChQcnKyQkJCFBERoSlTpuj06dPuLhUAAHgJ//pYaNeuXbV27dqfV+L/82oeeeQRffzxx3r77bcVFhamiRMn6q677tJnn30mSTpz5oySk5PldDr1+eef69ChQxozZoyaNm2qp556qj7KBQAAhquXQOPv7y+n03nB9NLSUv31r3/V8uXLdeutt0qS3njjDXXu3FlbtmzRjTfeqDVr1mjv3r1au3atIiMj1bNnT82ZM0fTpk3TrFmzFBAQUB8lAwAAg9XLNTRff/21oqOj1b59e6WkpKigoECSlJOTo8rKSiUmJtptO3XqpLZt2yo7O1uSlJ2drfj4eEVGRtptkpKSVFZWpj179lx0nadOnVJZWZnLAAAAfIPbA02/fv20ZMkSrVq1SosWLVJ+fr5uueUWHT16VIWFhQoICFB4eLjLayIjI1VYWChJKiwsdAkz1fOr513M3LlzFRYWZg8xMTHu3TAAANBouf0rp+HDh9v/7t69u/r166fY2Fi99dZbCg4OdvfqbNOnT1daWpo9XlZWRqgBAMBH1Ptt2+Hh4bruuuu0f/9+OZ1OVVRUqKSkxKVNUVGRfc2N0+m84K6n6vGarsupFhgYqNDQUJcBAAD4hnoPNOXl5frmm28UFRWl3r17q2nTpsrMzLTn5+XlqaCgQAkJCZKkhIQE7d69W8XFxXabjIwMhYaGqkuXLvVdLgAAMJDbv3L6n//5H40YMUKxsbE6ePCgZs6cKT8/P40aNUphYWEaN26c0tLS1KpVK4WGhuqhhx5SQkKCbrzxRknS0KFD1aVLF40ePVrz5s1TYWGhHnvsMaWmpiowMNDd5QIAAC/g9kDz/fffa9SoUfrpp5909dVX6+abb9aWLVt09dVXS5IWLFigJk2aaOTIkTp16pSSkpL05z//2X69n5+f0tPT9eCDDyohIUHNmjXT2LFjNXv2bHeXCgAAvITbA82KFSsuOT8oKEgLFy7UwoULL9omNjZWn3zyibtLAwAAXopnOQEAAOMRaAAAgPEINAAAwHgEGgAAYDwCDQAAMB6BBgAAGI9AAwAAjEegAQAAxiPQAAAA4xFoAACA8Qg0AADAeAQaAABgPAINAAAwHoEGAAAYj0ADAACMR6ABAADGI9AAAADjEWgAAIDxCDQAAMB4BBoAAGA8Ag0AADCev6cL8FbfBt17wbR2J5d7oBIA8DyOiahvBBr4pPo8uJ6/bA7auBg+5AH3IdD4sJoOplJpg9cBAMAvRaAB4FGcpWhcfPkMoy9vuzcg0HgBPhAAwPs0ZMC6knVd+FnTuM7oE2jqgABxefxPxzex3wF4CoEG8HKEjNrh2jLATAQaAF7Ll8Oct5xJbuxfc6DxINDAeL78oQWg8fOWcNnY8UvBAADAeAQaAABgPL5yAhoBLkQFgF+GMzQAAMB4BBoAAGA8Ag0AADAe19AAHlDzNTOeW7cv30LqyX0BwH0IND6CgzYAwJsRaOB1OAMBAL6HQIMG4a0ho7Gd+TKhHm/Y73XV2PZPY0P/NB4m7gsCjZs05M438Y2G+sF7AagbT//tNOQjW3zl8TAEmgbkyTcwPMeEfcF7s/Goz33hy/u5sdVTF/wA56URaPCLecOBAq5MPHB6+n3orf8Lrs/3Qn3tM3ctl69MzUKg8SATPzTqk4kHTtSPC/eXee+D+vxQNbE/cGkmvjcbGwINaq1+D9SeU18fogC8B8eJxotfCgYAAMYj0AAAAOMRaAAAgPEINAAAwHgEGgAAYDwCDQAAMB6BBgAAGK9RB5qFCxeqXbt2CgoKUr9+/bRt2zZPlwQAABqhRhtoVq5cqbS0NM2cOVM7duxQjx49lJSUpOLiYk+XBgAAGplGG2ief/55PfDAA7r//vvVpUsXLV68WCEhIXr99dc9XRoAAGhkGuWjDyoqKpSTk6Pp06fb05o0aaLExERlZ2fX+JpTp07p1KlT9nhp6dmfoy4rK3N/gacs9y+z2pXU683rP58n6/F0X3h6/edjX3hu/efzdD2+/F44n6fraez7ok6LPbtcy6pl3VYj9MMPP1iSrM8//9xl+pQpU6wbbrihxtfMnDnTksTAwMDAwMDgBcOBAwdqlR0a5Rmaupg+fbrS0tLs8aqqKh0+fFitW7eWw+Fw23rKysoUExOjAwcOKDQ01G3LNRF9cRb9cBb98DP64iz64Sz64WdX0heWZeno0aOKjo6u1bIbZaC56qqr5Ofnp6KiIpfpRUVFcjqdNb4mMDBQgYGBLtPCw8Prq0SFhob6/BuzGn1xFv1wFv3wM/riLPrhLPrhZ5fri7CwsFovs1FeFBwQEKDevXsrMzPTnlZVVaXMzEwlJCR4sDIAANAYNcozNJKUlpamsWPHqk+fPrrhhhv0wgsv6NixY7r//vs9XRoAAGhkGm2gufvuu/Xjjz9qxowZKiwsVM+ePbVq1SpFRkZ6tK7AwEDNnDnzgq+3fBF9cRb9cBb98DP64iz64Sz64Wf12RcOy6rtfVEAAACNS6O8hgYAAKA2CDQAAMB4BBoAAGA8Ag0AADAegaaWFi5cqHbt2ikoKEj9+vXTtm3bPF1SvZo7d6769u2rFi1aKCIiQnfeeafy8vJc2pw8eVKpqalq3bq1mjdvrpEjR17wo4je5umnn5bD4dCkSZPsab7UDz/88IPuu+8+tW7dWsHBwYqPj9cXX3xhz7csSzNmzFBUVJSCg4OVmJior7/+2oMVu9+ZM2f0+OOPKy4uTsHBwbrmmms0Z84cl+fPeGM/bNy4USNGjFB0dLQcDofef/99l/lXss2HDx9WSkqKQkNDFR4ernHjxqm8vLwBt8I9LtUXlZWVmjZtmuLj49WsWTNFR0drzJgxOnjwoMsyvKEvLveeONeECRPkcDj0wgsvuEx3Rz8QaGph5cqVSktL08yZM7Vjxw716NFDSUlJKi4u9nRp9WbDhg1KTU3Vli1blJGRocrKSg0dOlTHjh2z2zzyyCP66KOP9Pbbb2vDhg06ePCg7rrrLg9WXb+2b9+u//u//1P37t1dpvtKPxw5ckT9+/dX06ZN9emnn2rv3r2aP3++WrZsabeZN2+eXnrpJS1evFhbt25Vs2bNlJSUpJMnT3qwcvd65plntGjRIr3yyivat2+fnnnmGc2bN08vv/yy3cYb++HYsWPq0aOHFi5cWOP8K9nmlJQU7dmzRxkZGUpPT9fGjRs1fvz4htoEt7lUXxw/flw7duzQ448/rh07dujdd99VXl6ebr/9dpd23tAXl3tPVHvvvfe0ZcuWGh9p4JZ+qP2jI33XDTfcYKWmptrjZ86csaKjo625c+d6sKqGVVxcbEmyNmzYYFmWZZWUlFhNmza13n77bbvNvn37LElWdna2p8qsN0ePHrWuvfZaKyMjwxo4cKD18MMPW5blW/0wbdo06+abb77o/KqqKsvpdFrPPvusPa2kpMQKDAy0/v73vzdEiQ0iOTnZ+t3vfucy7a677rJSUlIsy/KNfpBkvffee/b4lWzz3r17LUnW9u3b7Taffvqp5XA4rB9++KHBane38/uiJtu2bbMkWd99951lWd7ZFxfrh++//9761a9+ZX355ZdWbGystWDBAnueu/qBMzRXqKKiQjk5OUpMTLSnNWnSRImJicrOzvZgZQ2rtLRUktSqVStJUk5OjiorK136pVOnTmrbtq1X9ktqaqqSk5NdtlfyrX748MMP1adPH/3Xf/2XIiIi1KtXL/3lL3+x5+fn56uwsNClL8LCwtSvXz+v6oubbrpJmZmZ+uc//ylJ+sc//qHNmzdr+PDhknynH851JducnZ2t8PBw9enTx26TmJioJk2aaOvWrQ1ec0MqLS2Vw+GwnzPoK31RVVWl0aNHa8qUKeratesF893VD432l4Ibm3//+986c+bMBb9UHBkZqa+++spDVTWsqqoqTZo0Sf3791e3bt0kSYWFhQoICLjgQaCRkZEqLCz0QJX1Z8WKFdqxY4e2b99+wTxf6od//etfWrRokdLS0vTHP/5R27dv1x/+8AcFBARo7Nix9vbW9LfiTX3x6KOPqqysTJ06dZKfn5/OnDmjJ598UikpKZLkM/1wrivZ5sLCQkVERLjM9/f3V6tWrby2X6Sz19hNmzZNo0aNsh/K6Ct98cwzz8jf319/+MMfapzvrn4g0OCKpaam6ssvv9TmzZs9XUqDO3DggB5++GFlZGQoKCjI0+V4VFVVlfr06aOnnnpKktSrVy99+eWXWrx4scaOHevh6hrOW2+9pWXLlmn58uXq2rWrcnNzNWnSJEVHR/tUP+DyKisr9Zvf/EaWZWnRokWeLqdB5eTk6MUXX9SOHTvkcDjqdV185XSFrrrqKvn5+V1w10pRUZGcTqeHqmo4EydOVHp6utavX682bdrY051OpyoqKlRSUuLS3tv6JScnR8XFxbr++uvl7+8vf39/bdiwQS+99JL8/f0VGRnpE/0gSVFRUerSpYvLtM6dO6ugoECS7O319r+VKVOm6NFHH9U999yj+Ph4jR49Wo888ojmzp0ryXf64VxXss1Op/OCGylOnz6tw4cPe2W/VIeZ7777ThkZGfbZGck3+mLTpk0qLi5W27Zt7WPnd999p8mTJ6tdu3aS3NcPBJorFBAQoN69eyszM9OeVlVVpczMTCUkJHiwsvplWZYmTpyo9957T+vWrVNcXJzL/N69e6tp06Yu/ZKXl6eCggKv6pchQ4Zo9+7dys3NtYc+ffooJSXF/rcv9IMk9e/f/4Jb9//5z38qNjZWkhQXFyen0+nSF2VlZdq6datX9cXx48fVpInrIdTPz09VVVWSfKcfznUl25yQkKCSkhLl5OTYbdatW6eqqir169evwWuuT9Vh5uuvv9batWvVunVrl/m+0BejR4/Wrl27XI6d0dHRmjJlilavXi3Jjf1Q92uZfc+KFSuswMBAa8mSJdbevXut8ePHW+Hh4VZhYaGnS6s3Dz74oBUWFmZlZWVZhw4dsofjx4/bbSZMmGC1bdvWWrdunfXFF19YCQkJVkJCggerbhjn3uVkWb7TD9u2bbP8/f2tJ5980vr666+tZcuWWSEhIdabb75pt3n66aet8PBw64MPPrB27dpl3XHHHVZcXJx14sQJD1buXmPHjrV+9atfWenp6VZ+fr717rvvWldddZU1depUu4039sPRo0etnTt3Wjt37rQkWc8//7y1c+dO+86dK9nmYcOGWb169bK2bt1qbd682br22mutUaNGeWqT6uxSfVFRUWHdfvvtVps2bazc3FyX4+epU6fsZXhDX1zuPXG+8+9ysiz39AOBppZefvllq23btlZAQIB1ww03WFu2bPF0SfVKUo3DG2+8Ybc5ceKE9d///d9Wy5YtrZCQEOvXv/61dejQIc8V3UDODzS+1A8fffSR1a1bNyswMNDq1KmT9eqrr7rMr6qqsh5//HErMjLSCgwMtIYMGWLl5eV5qNr6UVZWZj388MNW27ZtraCgIKt9+/bW//7v/7p8WHljP6xfv77GY8LYsWMty7qybf7pp5+sUaNGWc2bN7dCQ0Ot+++/3zp69KgHtuaXuVRf5OfnX/T4uX79ensZ3tAXl3tPnK+mQOOOfnBY1jk/awkAAGAgrqEBAADGI9AAAADjEWgAAIDxCDQAAMB4BBoAAGA8Ag0AADAegQYAABiPQAMAAIxHoAEAAMYj0AAAAOMRaAAAgPEINAAAwHj/D02SdkSMyozdAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors = []\n",
    "corrects = []\n",
    "for round_idx, rfs in enumerate(random_forests):\n",
    "    for block_idx, rfc in enumerate(rfs):\n",
    "        keyround_index = round_idx * BLOCK_WIDTH_B4 + block_idx\n",
    "        classifications_scores = rfc.predict(traces_test_reduced[round_idx, block_idx])\n",
    "        for trace_idx, seed in enumerate(seeds_test):\n",
    "            indices, whitening = chacha_random_b4(seed)\n",
    "            key_index = indices[keyround_index]\n",
    "            if (classifications_scores[trace_idx] - whitening[keyround_index]) % 16 != real_keys[0][key_index]:\n",
    "                errors.append((round_idx, block_idx))\n",
    "            else:\n",
    "                corrects.append((round_idx, block_idx))\n",
    "plt.hist([10 * r + b for r, b in errors], bins=100, label=\"Wrong\")\n",
    "plt.hist([10 * r + b for r, b in corrects], bins=100, label=\"Correct\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
