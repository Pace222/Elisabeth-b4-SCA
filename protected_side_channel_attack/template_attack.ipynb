{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.model_selection import train_test_split, ShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from typing import List\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces_path = \"..\\\\acquisition\\\\50000_same_varying-mask_nullmask_maskshuffle\\\\carto_eB4-Rnd-3-WhiteningAndFullFilter-Same-And-Varying-Multiple-Defenses.mat\"\n",
    "key_path = \"..\\\\acquisition\\\\50000_same_varying-mask_nullmask_maskshuffle\\\\carto_eB4-Rnd-3-WhiteningAndFullFilter-Same-And-Varying-Multiple-Defenses.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRACES = 50_000\n",
    "data_loader = EntireTraceIterator(traces_path, key_path, nr_populations=2, nr_scenarios=3, trace_size=850_000, traces_per_division=50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLIEST_ROUND = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_ALPHABET = list(range(16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sost_masks_card_m_v_y_subsample(seeds_sub: np.ndarray, key_shares_sub: np.ndarray, traces_sub: np.ndarray):\n",
    "    assert seeds_sub.shape[0] == traces_sub.shape[0] == key_shares_sub.shape[0]\n",
    "    assert key_shares_sub.shape[1:] == (KEY_WIDTH_B4, NR_SHARES)\n",
    "    card_g = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), 1), dtype=np.int32)\n",
    "    m = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), traces_sub.shape[1]), dtype=np.float32)\n",
    "    v = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), traces_sub.shape[1]), dtype=np.float64)\n",
    "    y = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, traces_sub.shape[0]), dtype=int)\n",
    "\n",
    "    for seed, key_shares, (trace_idx, trace) in zip(seeds_sub, key_shares_sub, enumerate(traces_sub)):\n",
    "        indices, whitening = chacha_random_b4(seed)\n",
    "\n",
    "        for round_idx in range(EARLIEST_ROUND, KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4):\n",
    "            for block_idx in range(BLOCK_WIDTH_B4):\n",
    "                keyround_index = round_idx * BLOCK_WIDTH_B4 + block_idx\n",
    "\n",
    "                shares = np.zeros(NR_SHARES, dtype=int)\n",
    "                for share_idx in range(shares.shape[0] - 1):\n",
    "                    shares[share_idx] = key_shares[indices[keyround_index], share_idx]\n",
    "                shares[-1] = (key_shares[indices[keyround_index], -1] + whitening[keyround_index]) % 16\n",
    "\n",
    "                for share_idx, share in enumerate(shares):\n",
    "                    card_g[round_idx, block_idx, share_idx, share, 0] += 1\n",
    "                    m[round_idx, block_idx, share_idx, share] += trace\n",
    "                    v[round_idx, block_idx, share_idx, share] += np.square(trace.astype(np.float64))\n",
    "\n",
    "                    y[round_idx, block_idx, share_idx, trace_idx] = share\n",
    "    \n",
    "    return card_g, m, v, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sost_combine_subsamples(card_g: np.ndarray, m: np.ndarray, v: np.ndarray) -> np.ndarray:\n",
    "    assert card_g.shape[3:] == (KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), 1)\n",
    "    assert m.ndim == 8 and m.shape[:7] == card_g.shape[:7]\n",
    "    assert m.shape == v.shape\n",
    "    f = np.zeros(m.shape[:-2] + (m.shape[-1],), dtype=np.float32)\n",
    "\n",
    "    m /= card_g\n",
    "    v = (v - card_g * m * m) / (card_g - 1)\n",
    "\n",
    "    for i in range(len(KEY_ALPHABET)):\n",
    "        for j in range(i + 1, len(KEY_ALPHABET)):\n",
    "            num = m[..., i] - m[..., j]\n",
    "            den = np.sqrt(v[..., i] / card_g[..., i] + v[..., j] / card_g[..., j])\n",
    "            f += np.square(num / den)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_largest_values_separated_by_cycle(f: np.ndarray, n: int, cycle_length: int) -> np.ndarray:\n",
    "    interesting_points_per_index = np.zeros((f.shape[0], f.shape[1], f.shape[2], n), dtype=int)\n",
    "    sorted_strengths = np.argsort(f, axis=3)[:, :, :, ::-1]\n",
    "\n",
    "    for round_idx in range(EARLIEST_ROUND, sorted_strengths.shape[0]):\n",
    "        for block_idx in range(sorted_strengths.shape[1]):\n",
    "            for share_idx in range(sorted_strengths.shape[2]):\n",
    "                largest_indices = []\n",
    "                for ind in sorted_strengths[round_idx, block_idx, share_idx]:\n",
    "                    if len(largest_indices) < n:\n",
    "                        if all(abs(i - ind) >= cycle_length for i in largest_indices):\n",
    "                            largest_indices.append(ind)\n",
    "                    else:\n",
    "                        break\n",
    "                interesting_points_per_index[round_idx, block_idx, share_idx] = np.array(largest_indices)\n",
    "    \n",
    "    return interesting_points_per_index\n",
    "\n",
    "def n_largest_values(f: np.ndarray, n: int) -> np.ndarray:\n",
    "    interesting_points_per_index = np.apply_along_axis(np.argpartition, axis=3, arr=f, kth=-n)[:, :, :, -n:]\n",
    "    return interesting_points_per_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError\n",
    "def means_and_covariance_large_dataset(data_loader: EntireTraceIterator, interesting_points_per_index: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    total_samples = np.zeros((len(KEY_ALPHABET),) + (interesting_points_per_index.shape[:3]), dtype=int)\n",
    "    mean_vector = np.zeros(total_samples.shape + (interesting_points_per_index.shape[3],), dtype=np.float32)\n",
    "    squared_dist_from_mean = np.zeros(total_samples.shape + (interesting_points_per_index.shape[3], interesting_points_per_index.shape[3]), dtype=np.float64)\n",
    "\n",
    "    for seeds_sub, traces_sub, key, key_shares_sub in data_loader:\n",
    "        traces_reduced = traces_sub[:, interesting_points_per_index].transpose(1, 2, 3, 0, 4)\n",
    "        for k in len(KEY_ALPHABET):\n",
    "            traces_reduced_per_key = traces_reduced[:, :, :, np.repeat((y == k)[:, :, :, :, np.newaxis], traces_reduced.shape[-1], axis=-1)]#.reshape(-1, traces_reduced.shape[-1])\n",
    "                        \n",
    "            # Welford's online algorithm\n",
    "            total_samples[k] += traces_reduced_per_key.shape[-2]\n",
    "            delta = np.subtract(traces_reduced_per_key, mean_vector[k])\n",
    "            mean_vector[k] += np.sum(delta / total_samples[k])\n",
    "            delta2 = np.subtract(traces_reduced_per_key, mean_vector[k])\n",
    "            squared_dist_from_mean[k] = np.sum(delta * delta2)\n",
    "\n",
    "    return np.moveaxis(mean_vector, 0, -1), np.moveaxis(squared_dist_from_mean / (total_samples - 1), 0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_key_count_best(classifications_scores: np.ndarray, target_seeds: np.ndarray):\n",
    "    classifications_best = np.argmax(classifications_scores, axis=-1)\n",
    "\n",
    "    classifications_per_key_nibble = np.zeros((KEY_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET)), dtype=int)\n",
    "    for i, seed in enumerate(target_seeds):\n",
    "        indices, whitening = chacha_random_b4(seed)\n",
    "        for keyround_index in range(KEYROUND_WIDTH_B4):\n",
    "            key_index = indices[keyround_index]\n",
    "            round_idx = keyround_index // BLOCK_WIDTH_B4\n",
    "            block_idx = keyround_index % BLOCK_WIDTH_B4\n",
    "\n",
    "            if round_idx >= EARLIEST_ROUND:\n",
    "                for share_idx in range(NR_SHARES - 1):\n",
    "                    classifications_per_key_nibble[key_index, share_idx, classifications_best[round_idx, block_idx, share_idx, i]] += 1\n",
    "                classifications_per_key_nibble[key_index, NR_SHARES - 1, (classifications_best[round_idx, block_idx, NR_SHARES - 1, i] - whitening[keyround_index]) % 16] += 1\n",
    "\n",
    "    recovered_key = np.argmax(classifications_per_key_nibble, axis=2) # Majority voting\n",
    "    return np.sum(recovered_key, axis=1) % 16\n",
    "\n",
    "def recover_key_sum_probs_rank(classifications_scores: np.ndarray, target_seeds: np.ndarray):\n",
    "    classifications_ranks = np.apply_along_axis(lambda a: np.searchsorted(np.sort(-a), -a), axis=-1, arr=classifications_scores)\n",
    "\n",
    "    classifications_per_key_nibble = np.zeros((KEY_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET)), dtype=int)\n",
    "    for i, seed in enumerate(target_seeds):\n",
    "        indices, whitening = chacha_random_b4(seed)\n",
    "        for keyround_index in range(KEYROUND_WIDTH_B4):\n",
    "            key_index = indices[keyround_index]\n",
    "            round_idx = keyround_index // BLOCK_WIDTH_B4\n",
    "            block_idx = keyround_index % BLOCK_WIDTH_B4\n",
    "\n",
    "            if round_idx >= EARLIEST_ROUND:\n",
    "                for share_idx in range(NR_SHARES - 1):\n",
    "                    classifications_per_key_nibble[key_index, share_idx] += classifications_ranks[round_idx, block_idx, share_idx, i]\n",
    "                classifications_per_key_nibble[key_index, NR_SHARES - 1] += np.roll(classifications_ranks[round_idx, block_idx, NR_SHARES - 1, i], -whitening[keyround_index])\n",
    "\n",
    "    recovered_key = np.argmin(classifications_per_key_nibble, axis=2)\n",
    "    return np.sum(recovered_key, axis=1) % 16\n",
    "\n",
    "def recover_key_multiply_probs(classifications_scores: np.ndarray, target_seeds: np.ndarray):\n",
    "    classifications_per_key_nibble = np.zeros((KEY_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET)), dtype=np.longdouble)\n",
    "    for i, seed in enumerate(target_seeds):\n",
    "        indices, whitening = chacha_random_b4(seed)\n",
    "        for keyround_index in range(KEYROUND_WIDTH_B4):\n",
    "            key_index = indices[keyround_index]\n",
    "            round_idx = keyround_index // BLOCK_WIDTH_B4\n",
    "            block_idx = keyround_index % BLOCK_WIDTH_B4\n",
    "\n",
    "            if round_idx >= EARLIEST_ROUND:\n",
    "                for share_idx in range(NR_SHARES - 1):\n",
    "                    classifications_per_key_nibble[key_index, share_idx] += classifications_scores[round_idx, block_idx, share_idx, i]\n",
    "                classifications_per_key_nibble[key_index, NR_SHARES - 1] += np.roll(classifications_scores[round_idx, block_idx, NR_SHARES - 1, i], -whitening[keyround_index])\n",
    "\n",
    "    recovered_key = np.argmax(classifications_per_key_nibble, axis=2)\n",
    "    return np.sum(recovered_key, axis=1) % 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"card_g = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), 1), dtype=np.int32)\n",
    "m = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), data_loader.trace_size), dtype=np.float32)\n",
    "v = np.zeros((KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), data_loader.trace_size), dtype=np.float64)\n",
    "y = []\n",
    "\n",
    "for seeds_sub, traces_sub, key, key_shares_sub in data_loader((1,), (0,)):\n",
    "    card_g_sub, m_sub, v_sub, y_sub = sost_masks_card_m_v_y_subsample(seeds_sub[0][0], key_shares_sub[0][0], traces_sub[0][0])\n",
    "    card_g += card_g_sub\n",
    "    m += m_sub\n",
    "    v += v_sub\n",
    "    y.append(y_sub)\n",
    "\n",
    "y = np.concatenate(y, axis=3)\n",
    "f = sost_combine_subsamples(card_g, m, v)\n",
    "with open(\"card_m_v_y_f_2_shares_masking.pic\", \"wb\") as w:\n",
    "    pic.dump((card_g, m, v, y, f), w)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 1\n",
    "train_sizes = [45_000]\n",
    "val_size_grid = [5_000]\n",
    "\n",
    "grid = {\"num_features\": [40]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_g = np.zeros((len(train_sizes), len(val_size_grid), n_folds, KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), 1), dtype=np.int32)\n",
    "m = np.zeros((len(train_sizes), len(val_size_grid), n_folds, KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), data_loader.trace_size), dtype=np.float32)\n",
    "v = np.zeros((len(train_sizes), len(val_size_grid), n_folds, KEYROUND_WIDTH_B4 // BLOCK_WIDTH_B4, BLOCK_WIDTH_B4, NR_SHARES, len(KEY_ALPHABET), data_loader.trace_size), dtype=np.float64)\n",
    "y = [[[[] for cv in range(n_folds)] for m in range(len(val_size_grid))] for ts in range(len(train_sizes))]\n",
    "\n",
    "for i, (seeds_sub, traces_sub, key, key_shares_sub) in enumerate(data_loader((1,), (0,))):\n",
    "    for ts, train_size in enumerate(train_sizes):\n",
    "        for m, val_size in enumerate(val_size_grid):\n",
    "            rs = ShuffleSplit(n_splits=n_folds, test_size=val_size*data_loader.step//NUM_TRACES, train_size=train_size*data_loader.step//NUM_TRACES, random_state=i)\n",
    "            for cv, (train_index, val_index) in enumerate(rs.split(traces_sub[0][0])):\n",
    "                traces_train, seeds_train = traces_sub[0][0][train_index], seeds_sub[0][0][train_index]\n",
    "                card_g_sub, m_sub, v_sub, y_sub = sost_masks_card_m_v_y_subsample(seeds_train, key_shares_sub[0][0], traces_train)\n",
    "                card_g[ts, m, cv] += card_g_sub\n",
    "                m[ts, m, cv] += m_sub\n",
    "                v[ts, m, cv] += v_sub\n",
    "                y[ts][m][cv].append(y_sub)\n",
    "\n",
    "y = np.concatenate(y, axis=-1)\n",
    "f = sost_combine_subsamples(card_g, m, v)\n",
    "with open(\"card_m_v_y_f_2_shares_masking.pic\", \"wb\") as w:\n",
    "    pic.dump((card_g, m, v, y, f), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_largest_values(f: np.ndarray, n: int) -> np.ndarray:\n",
    "    interesting_points_per_index = np.apply_along_axis(np.argpartition, axis=2, arr=f, kth=-n)[:, :, -n:]\n",
    "    return interesting_points_per_index\n",
    "\n",
    "\n",
    "best_accuracy = -1\n",
    "grid_results = np.zeros((len(grid[\"num_features\"]), len(train_sizes), len(val_size_grid), f_trainval_tot.shape[1], n_folds), dtype=np.float64)\n",
    "for l, num_features in enumerate(grid[\"num_features\"]):\n",
    "    for ts, train_size in enumerate(train_sizes):\n",
    "        f_trainval_methods = f_trainval_tot[ts]\n",
    "        y_trainval = y_trainval_tot[ts]\n",
    "\n",
    "        for m, val_size in enumerate(val_size_grid):\n",
    "            rs = ShuffleSplit(n_splits=n_folds, test_size=val_size, train_size=train_size, random_state=0)\n",
    "            times = np.zeros((2, f_trainval_methods.shape[0], n_folds), dtype=np.float64)\n",
    "            for cv, (train_index, val_index) in enumerate(rs.split(traces_trainval[ts])):\n",
    "                print(f\"CV {cv}\", end=\"\\r\")\n",
    "                traces_train, traces_val = traces_trainval[ts][train_index], traces_trainval[ts][val_index]\n",
    "                seeds_train, seeds_val = seeds_trainval[ts][train_index], seeds_trainval[ts][val_index]\n",
    "                y = y_trainval[m, cv]\n",
    "\n",
    "                for n, f_trainval in enumerate(f_trainval_methods):\n",
    "                    f = f_trainval[m, cv]\n",
    "\n",
    "                    interesting_points_per_index = n_largest_values(f, num_features)\n",
    "                    traces_reduced = traces_train[:, interesting_points_per_index].transpose(1, 2, 0, 3)\n",
    "                    traces_val_reduced = traces_val[:, interesting_points_per_index].transpose(1, 2, 0, 3)\n",
    "\n",
    "                    traces_reduced_per_key = [[[traces_reduced[round_idx, block_idx, np.repeat((y == k)[round_idx, block_idx, :, np.newaxis], traces_reduced.shape[-1], axis=-1)].reshape(-1, traces_reduced.shape[-1]) for k in range(len(KEY_ALPHABET))] for block_idx in range(traces_reduced.shape[1])] for round_idx in range(traces_reduced.shape[0])]\n",
    "\n",
    "                    start = time()\n",
    "                    template_means = np.array([[[np.mean(traces_reduced_per_key[round_idx][block_idx][k], axis=-2) for k in range(len(KEY_ALPHABET))] for block_idx in range(traces_reduced.shape[1])] for round_idx in range(traces_reduced.shape[0])])\n",
    "                    template_covariances = np.array([[[np.nan_to_num(np.cov(traces_reduced_per_key[round_idx][block_idx][k], rowvar=False), nan=0.0) for k in range(template_means.shape[2])] for block_idx in range(template_means.shape[1])] for round_idx in range(template_means.shape[0])])\n",
    "                    pdfs = np.array([[[multivariate_normal(template_means[round_idx, block_idx, k], template_covariances[round_idx, block_idx, k], allow_singular=True) for k in range(template_means.shape[2])] for block_idx in range(template_means.shape[1])] for round_idx in range(template_means.shape[0])])\n",
    "                    times[0, n, cv] = time() - start\n",
    "                    \n",
    "\n",
    "                    start = time()\n",
    "                    val_probas = np.array([[[[pdfs[round_idx, block_idx, key_guess].logpdf(traces_val_reduced[round_idx, block_idx, trace]) for key_guess in range(pdfs.shape[2])] for trace in range(traces_val_reduced.shape[2])] for block_idx in range(pdfs.shape[1])] for round_idx in range(pdfs.shape[0])])\n",
    "                    recovered_key = recover_key_multiply_probs(val_probas, seeds_val)\n",
    "                    times[1, n, cv] = time() - start\n",
    "\n",
    "                    grid_results[l, ts, m, n, cv] = np.count_nonzero(recovered_key == real_keys[0]) / KEY_WIDTH_B4\n",
    "                \n",
    "            print(f\"SOST [num features: {num_features}, train size: {train_size}, val size: {val_size}]: {np.mean(grid_results[l, ts, m, 0]):#.4g} ± {np.std(grid_results[l, ts, m, 0]):#.4g} ({grid_results[l, ts, m, 0]}). Training in {np.mean(times[0, 0]):#.4g} ± {np.std(times[0, 0]):#.4g} seconds. Extracting in {np.mean(times[1, 0]):#.4g} ± {np.std(times[1, 0]):#.4g} seconds.\")\n",
    "            if np.mean(grid_results[l, ts, m, 0]) > best_accuracy:\n",
    "                print(\"New best model found ! (Above)\")\n",
    "                best_accuracy = np.mean(grid_results[l, ts, m, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"card_m_v_y_f_2_shares_masking.pic\", \"rb\") as r:\n",
    "    _, _, _, y_trainval_tot, f_trainval_tot = pic.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results = np.zeros((len(grid[\"num_features\"]), len(train_sizes), len(val_size_grid), n_folds), dtype=np.float64)\n",
    "\n",
    "best_accuracy = -1\n",
    "for l, num_features in enumerate(grid[\"num_features\"]):\n",
    "    for ts, train_size in enumerate(train_sizes):\n",
    "        f_trainval = f_trainval_tot[ts]\n",
    "        y_trainval = y_trainval_tot[ts]\n",
    "        for m, val_size in enumerate(val_size_grid):\n",
    "            traces_train_reduced = np.zeros((n_folds, train_size, num_features), dtype=np.float32)\n",
    "            traces_val_reduced = np.zeros((n_folds, val_size, num_features), dtype=np.float32)\n",
    "            seeds_val = np.zeros((n_folds, val_size), dtype=\"<U32\")\n",
    "            for i, (seeds_sub, traces_sub, key, key_shares_sub) in enumerate(data_loader((1,), (0,))):\n",
    "                real_key = key\n",
    "                rs = ShuffleSplit(n_splits=n_folds, test_size=val_size*data_loader.step//NUM_TRACES, train_size=train_size*data_loader.step//NUM_TRACES, random_state=i)\n",
    "                for cv, (train_index, val_index) in enumerate(rs.split(traces_sub[0][0])):\n",
    "                    traces_train, traces_val = traces_sub[0][0][train_index], traces_sub[0][0][val_index]\n",
    "                    y = y_trainval[m, cv]\n",
    "                    f = f_trainval[m, cv]\n",
    "                    interesting_points_per_index = n_largest_values(f, num_features)\n",
    "\n",
    "                    traces_train_reduced[cv][i*data_loader.step//NUM_TRACES:(i+1)*data_loader.step//NUM_TRACES] = traces_train[:, interesting_points_per_index].transpose(1, 2, 3, 0, 4)\n",
    "                    traces_val_reduced  [cv][i*data_loader.step//NUM_TRACES:(i+1)*data_loader.step//NUM_TRACES] = traces_val[:, interesting_points_per_index].transpose(1, 2, 3, 0, 4)\n",
    "                    seeds_val[cv][i*data_loader.step//NUM_TRACES:(i+1)*data_loader.step//NUM_TRACES] = seeds_sub[0][0\n",
    "\n",
    "            times = np.zeros((2, n_folds), dtype=np.float64)\n",
    "            for cv in range(n_folds):\n",
    "                start = time()\n",
    "                for k in len(KEY_ALPHABET):\n",
    "                    traces_reduced_per_key = traces_train_reduced[cv][..., np.repeat((y == k)[..., np.newaxis], traces_train_reduced.shape[-1], axis=-1)]#.reshape(-1, traces_reduced.shape[-1])\n",
    "\n",
    "                    template_means = np.mean(traces_reduced_per_key[..., k], axis=-2)\n",
    "                    template_covariances = np.array([[np.nan_to_num(np.cov(traces_reduced_per_key[round_idx][block_idx][k], rowvar=False), nan=0.0) for block_idx in range(template_means.shape[1])] for round_idx in range(template_means.shape[0])])\n",
    "                    pdfs = np.array([[multivariate_normal(template_means[round_idx, block_idx, k], template_covariances[round_idx, block_idx, k], allow_singular=True) for block_idx in range(template_means.shape[1])] for round_idx in range(template_means.shape[0])])\n",
    "                times[0, cv] = time() - start\n",
    "                \n",
    "                start = time()\n",
    "                val_probas = np.array([[[[pdfs[round_idx, block_idx, key_guess].logpdf(traces_val_reduced[cv, round_idx, block_idx, trace]) for key_guess in range(pdfs.shape[2])] for trace in range(traces_val_reduced.shape[2])] for block_idx in range(pdfs.shape[1])] for round_idx in range(pdfs.shape[0])])\n",
    "                recovered_key = recover_key_multiply_probs(val_probas, seeds_val[cv])\n",
    "                times[1, cv] = time() - start\n",
    "\n",
    "                grid_results[l, ts, m, cv] = np.count_nonzero(recovered_key == real_keys[0]) / KEY_WIDTH_B4\n",
    "                \n",
    "            print(f\"SOST [num features: {num_features}, train size: {train_size}, val size: {val_size}]: {np.mean(grid_results[l, ts, m]):#.4g} ± {np.std(grid_results[l, ts, m]):#.4g} ({grid_results[l, ts, m]}). Training in {np.mean(times[0]):#.4g} ± {np.std(times[0]):#.4g} seconds. Extracting in {np.mean(times[1]):#.4g} ± {np.std(times[1]):#.4g} seconds.\")\n",
    "            if np.mean(grid_results[l, ts, m]) > best_accuracy:\n",
    "                print(\"New best model found ! (Above)\")\n",
    "                best_accuracy = np.mean(grid_results[l, ts, m])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
